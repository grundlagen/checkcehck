{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1T2cG4Y6_bF4hoBv2h3cNhhPbKPybR-2j","authorship_tag":"ABX9TyN9N32Q9KTc3NZr/2FYw3on"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_yfjUK_wzTw","executionInfo":{"status":"ok","timestamp":1729005820665,"user_tz":-60,"elapsed":1384,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"4f19b714-9dbb-47db-efef-69ab507daf28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting epitran\n","  Using cached epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement speech_recognition (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for speech_recognition\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install epitran gensim speech_recognition tdqm"]},{"cell_type":"code","source":["!pip install epitran"],"metadata":{"id":"1rtHCkVVxb2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729005842183,"user_tz":-60,"elapsed":7283,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"65ec71a2-c1f6-445f-dd70-67363bddb3e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting epitran\n","  Using cached epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (71.0.4)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2024.9.11)\n","Collecting panphon>=0.20 (from epitran)\n","  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: marisa-trie in /usr/local/lib/python3.10/dist-packages (from epitran) (1.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.32.3)\n","Collecting jamo (from epitran)\n","  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n","Collecting g2pk (from epitran)\n","  Downloading g2pK-0.9.4-py3-none-any.whl.metadata (7.5 kB)\n","Collecting unicodecsv (from panphon>=0.20->epitran)\n","  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0.2)\n","Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.26.4)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.8.1)\n","Collecting munkres (from panphon>=0.20->epitran)\n","  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from g2pk->epitran) (3.8.1)\n","Collecting konlpy (from g2pk->epitran)\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting python-mecab-ko (from g2pk->epitran)\n","  Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2024.8.30)\n","Collecting JPype1>=0.7.0 (from konlpy->g2pk->epitran)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->g2pk->epitran) (4.9.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (1.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (4.66.5)\n","Collecting python-mecab-ko-dic (from python-mecab-ko->g2pk->epitran)\n","  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->g2pk->epitran) (24.1)\n","Downloading epitran-1.25.1-py2.py3-none-any.whl (184 kB)\n","Downloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n","Downloading g2pK-0.9.4-py3-none-any.whl (27 kB)\n","Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n","Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: unicodecsv\n","  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=de606d7befaa69527c214c7ddb436662b2ed9e63958c1dc892d0b61c6309ff98\n","  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n","Successfully built unicodecsv\n","Installing collected packages: unicodecsv, python-mecab-ko-dic, munkres, jamo, python-mecab-ko, panphon, JPype1, konlpy, g2pk, epitran\n","Successfully installed JPype1-1.5.0 epitran-1.25.1 g2pk-0.9.4 jamo-0.4.1 konlpy-0.6.0 munkres-1.1.4 panphon-0.21.2 python-mecab-ko-1.3.7 python-mecab-ko-dic-2.1.1.post2 unicodecsv-0.14.1\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vFphln98xom0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade pip"],"metadata":{"id":"rciPJ0-JxeSn","executionInfo":{"status":"ok","timestamp":1729005748794,"user_tz":-60,"elapsed":4437,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"d5369bfa-3750-4026-a9e6-6ef0afa5e4bf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n","Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-24.2\n"]}]},{"cell_type":"code","source":["!pip install tdqm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24i0hLU2xBi2","executionInfo":{"status":"ok","timestamp":1729005827875,"user_tz":-60,"elapsed":2853,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"c15aac64-4bdd-4c72-fbd4-0e9ed35d22b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tdqm in /usr/local/lib/python3.10/dist-packages (0.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tdqm) (4.66.5)\n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GllsgtRlxKww","executionInfo":{"status":"ok","timestamp":1729005831020,"user_tz":-60,"elapsed":3151,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"e1963f29-113a-4dc1-d73a-e671a7273d3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"]}]},{"cell_type":"code","source":["import itertools\n","\n","from gensim.models import Word2Vec\n","import numpy as np\n","import epitran\n","from tqdm import tqdm\n","\n","# Define articulatory features for phonemes\n","articulatory_features = {\n","    't': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'ʔ': {'place': 'glottal', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'd': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiced'},\n","    's': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'z': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'ə': {'place': 'central', 'manner': 'vowel', 'voicing': 'voiced'},\n","    'ʃ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ʒ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'p': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'b': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiced'},\n","    'k': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'g': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiced'},\n","    'f': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'v': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'θ': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ð': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'm': {'place': 'bilabial', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'n': {'place': 'alveolar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'ŋ': {'place': 'velar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'h': {'place': 'glottal', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'j': {'place': 'palatal', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'w': {'place': 'labio-velar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'r': {'place': 'alveolar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'l': {'place': 'alveolar', 'manner': 'lateral approximant', 'voicing': 'voiced'},\n","    # Add more phonemes...\n","}\n","\n","# Phoneme mapping between similar sounds\n","phoneme_mapping = {\n","    'i': ['i', 'ɪ'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ'],\n","    'ɛ': ['ɛ', 'e'],\n","    'æ': ['a', 'æ'],\n","    'ɑ': ['a', 'ɑ'],\n","    'ɔ': ['o', 'ɔ'],\n","    'ə': ['ə', 'ʌ'],\n","    'ʌ': ['ə', 'ʌ'],\n","    't': ['t', 'ʔ', 'ɾ'],\n","    'd': ['t', 'ɾ'],\n","    's': ['s', 'z'],\n","    'z': ['s', 'z'],\n","    'ʃ': ['ʃ', 'ʒ'],\n","    'ʒ': ['ʃ', 'ʒ'],\n","    'n': ['n', 'ŋ'],\n","    'ŋ': ['n', 'ŋ'],\n","    'l': ['l', 'ɫ'],\n","    'r': ['r', 'ɹ'],\n","    'p': ['p', 'b'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g'],\n","    'g': ['k', 'g'],\n","    'f': ['f', 'v'],\n","    'v': ['f', 'v'],\n","    'θ': ['θ', 'ð'],\n","    'ð': ['θ', 'ð'],\n","    'h': ['h', 'ʔ'],\n","    'w': ['w', 'v'],\n","    # Add more based on phonetic similarities\n","}\n","\n","# Function to extract phonemes from a word using Epitran\n","def get_phonemes(word):\n","    epi = epitran.Epitran('eng-Latn')\n","    return epi.transliterate(word)\n","\n","# Function to merge phonemes with a gap represented as a glottal stop or ignoring the gap\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=False):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find homophones with gaps, including a progress bar and partial word matching\n","# Now includes printing matches in batches and using prosody tricks\n","def find_homophones_with_gaps(word_pairs, ignore_gap=False):\n","    homophones = []\n","    batch_size = 10\n","    for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding homophones with gaps\")):\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=ignore_gap)\n","        for other_word in vocabulary:\n","            other_phonemes = get_phonemes(other_word)\n","            # Check if merged phonemes match full or partially match\n","            if merged_phonemes == other_phonemes or merged_phonemes in other_phonemes or other_phonemes in merged_phonemes:\n","                homophones.append((f\"{word1} {word2}\", other_word))\n","        # Print matches in batches\n","        if (idx + 1) % batch_size == 0:\n","            print(f\"Matches for batch {idx // batch_size + 1}:\")\n","            for match in homophones[-batch_size:]:\n","                print(match)\n","    # Print final matches\n","    if len(homophones) % batch_size != 0:\n","        print(\"Final Matches:\")\n","        for match in homophones[-(len(homophones) % batch_size):]:\n","            print(match)\n","    return homophones\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence)\n","print(\"Phoneme Variants:\", sequences)\n","\n","# Function to use speech recognition and extract phonemes\n","def transcribe_audio_with_alternatives(audio_file):\n","    recognizer = sr.Recognizer()\n","    with sr.AudioFile(audio_file) as source:\n","        audio = recognizer.record(source)\n","    # Recognize with phoneme-level details (if supported)\n","    result = recognizer.recognize_google(audio, show_all=True)\n","    return result\n","\n","# Function to train Word2Vec on phoneme sequences, including a progress bar\n","def train_phoneme_embeddings(vocabulary):\n","    phoneme_sequences = [list(get_phonemes(word)) for word in tqdm(vocabulary, desc=\"Generating phoneme sequences\")]\n","    model = Word2Vec(phoneme_sequences, vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = ['sea', 'see', 'pain', 'wine', 'key', 'sun', 'main', 'air', 'bear', 'belle', 'pie', 'cat', 'chat', 'father', 'mere']\n","phoneme_model = train_phoneme_embeddings(vocabulary)\n","embedding = phoneme_model.wv['t']\n","print(\"Embedding for 't':\", embedding)\n","\n","# Function to align two texts at the phoneme level\n","def align_texts_with_prosody(text1, text2):\n","    words1 = text1.split()\n","    words2 = text2.split()\n","    phonemes1 = [get_phonemes(word) for word in words1]\n","    phonemes2 = [get_phonemes(word) for word in words2]\n","    alignment = dynamic_phoneme_alignment(phonemes1, phonemes2)\n","    combined_text = generate_combined_text(alignment)\n","    return combined_text\n","\n","# Placeholder function for dynamic programming alignment\n","def dynamic_phoneme_alignment(seq1, seq2):\n","    # Implement dynamic programming alignment\n","    pass\n","\n","# Rule 1: Vowel Length Matching\n","def match_vowel_length(phoneme1, phoneme2):\n","    return phoneme1 in ['iː', 'i'] and phoneme2 in ['iː', 'i']\n","\n","# Rule 2: Aspiration and Fricative Interaction\n","def aspiration_and_fricative_interaction(phoneme1, phoneme2):\n","    return (phoneme1 == 'h' and phoneme2 == 'ʃ') or (phoneme1 == 'ʃ' and phoneme2 == 'h')\n","\n","# Rule 3: Schwa Neutralization\n","def schwa_neutralization(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 4: Glottal Stop Substitution\n","def glottal_stop_substitution(phoneme1, phoneme2):\n","    return (phoneme1 == 'ʔ' and phoneme2 == 't') or (phoneme1 == 't' and phoneme2 == 'ʔ')\n","\n","# Rule 5: Voicing Neutralization\n","def voicing_neutralization(phoneme1, phoneme2):\n","    voicing_pairs = [('s', 'z'), ('f', 'v'), ('θ', 'ð')]\n","    return (phoneme1, phoneme2) in voicing_pairs or (phoneme2, phoneme1) in voicing_pairs\n","\n","# Rule 6: Nasal Consonant Alternation\n","def nasal_consonant_alternation(phoneme1, phoneme2):\n","    return (phoneme1 == 'n' and phoneme2 == 'ŋ') or (phoneme1 == 'ŋ' and phoneme2 == 'n')\n","\n","# Rule 7: Alveolar Tap\n","def alveolar_tap(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 8: Vowel Reduction\n","def vowel_reduction(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 9: Place Assimilation\n","def place_assimilation(phoneme1, phoneme2, next_phoneme):\n","    return (phoneme1 == 'n' and next_phoneme == 'k' and phoneme2 == 'ŋ')\n","\n","# Rule 10: Lateral Consonant Variation\n","def lateral_consonant_variation(phoneme1, phoneme2):\n","    return (phoneme1 == 'l' and phoneme2 == 'ɫ') or (phoneme1 == 'ɫ' and phoneme2 == 'l')\n","\n","# Rule 11: Diphthong Simplification\n","def diphthong_simplification(phoneme1, phoneme2):\n","    return (phoneme1 == 'eɪ' and phoneme2 == 'e') or (phoneme1 == 'e' and phoneme2 == 'eɪ')\n","\n","# Rule 12: R-coloring\n","def r_coloring(phoneme1, phoneme2):\n","    return (phoneme1 == 'ɑr' and phoneme2 == 'ɝː') or (phoneme1 == 'ɝː' and phoneme2 == 'ɑr')\n","\n","# Rule 13: Voicing Assimilation\n","def voicing_assimilation(phoneme1, surrounding_phonemes):\n","    return phoneme1 in ['b', 'd', 'g'] and any(p in ['s', 't', 'k'] for p in surrounding_phonemes)\n","\n","# Rule 14: Glottal Replacement for Gaps\n","def glottal_replacement_for_gaps(phoneme1, phoneme2):\n","    return phoneme1 == 'ʔ' or phoneme2 == 'ʔ'\n","\n","# Rule 15: Deletion of Unstressed Syllables\n","def deletion_of_unstressed_syllables(word_phonemes):\n","    return [p for p in word_phonemes if p != 'ə']\n","\n","# Rule 16: Palatalization\n","def palatalization(phoneme1, next_phoneme):\n","    return phoneme1 == 't' and next_phoneme == 'j'\n","\n","# Rule 17: Flapping\n","def flapping(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 18: Coalescence of Alveolar and Palatal\n","def coalescence_alveolar_palatal(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ʃ') or (phoneme1 == 'd' and phoneme2 == 'ʒ')\n","\n","# Rule 19: Velar Fronting\n","def velar_fronting(phoneme1, next_phoneme):\n","    return phoneme1 == 'k' and next_phoneme in ['i', 'e']\n","\n","# Rule 20: Vowel Harmony\n","def vowel_harmony(phoneme1, phoneme2):\n","    return phoneme1 in ['i', 'e'] and phoneme2 in ['i', 'e']\n","\n","# Rule 21: Devoicing of Final Consonants\n","def devoicing_final_consonants(phoneme, position):\n","    return position == 'final' and phoneme in ['b', 'd', 'g']\n","\n","# Rule 22: Nasal Place Assimilation\n","def nasal_place_assimilation(phoneme1, next_phoneme):\n","    return phoneme1 == 'n' and next_phoneme in ['p', 'b', 'm']\n","\n","# Rule 23: Geminate Reduction\n","def geminate_reduction(phoneme_sequence):\n","    return [phoneme_sequence[i] for i in range(len(phoneme_sequence)) if i == 0 or phoneme_sequence[i] != phoneme_sequence[i-1]]\n","\n","# Rule 24: Consonant Cluster Simplification\n","def consonant_cluster_simplification(cluster):\n","    return [cluster[0]] if len(cluster) > 1 else cluster"],"metadata":{"id":"sQjjo-FlxOEz","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1729005852748,"user_tz":-60,"elapsed":1270,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"b5fdb28a-7f71-4b3e-a07b-0c4f50e1d3e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Phoneme Variants: ['pətər', 'pəʔər', 'pəɾər', 'pʌtər', 'pʌʔər', 'pʌɾər', 'bətər', 'bəʔər', 'bəɾər', 'bʌtər', 'bʌʔər', 'bʌɾər']\n"]},{"output_type":"stream","name":"stderr","text":["\rGenerating phoneme sequences:   0%|          | 0/15 [00:00<?, ?it/s]WARNING:epitran:lex_lookup (from flite) is not installed.\n"]},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-a7f015fac053>\u001b[0m in \u001b[0;36m<cell line: 141>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Example usage of Word2Vec for phonemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sea'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'see'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'air'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'belle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pie'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'father'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mere'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mphoneme_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoneme_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding for 't':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-a7f015fac053>\u001b[0m in \u001b[0;36mtrain_phoneme_embeddings\u001b[0;34m(vocabulary)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# Function to train Word2Vec on phoneme sequences, including a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mphoneme_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating phoneme sequences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneme_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-a7f015fac053>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# Function to train Word2Vec on phoneme sequences, including a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mphoneme_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating phoneme sequences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneme_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-a7f015fac053>\u001b[0m in \u001b[0;36mget_phonemes\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mepi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepitran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpitran\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng-Latn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mepi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Function to merge phonemes with a gap represented as a glottal stop or ignoring the gap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/epitran/_epitran.py\u001b[0m in \u001b[0;36mtransliterate\u001b[0;34m(self, word, normpunc, ligatures)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormpunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mligatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreverse_transliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/epitran/flite.py\u001b[0m in \u001b[0;36mtransliterate\u001b[0;34m(self, text, normpunc, ligatures)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mletter_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_g2p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/epitran/flite.py\u001b[0m in \u001b[0;36menglish_g2p\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# Split on newlines and take the first element (in case lex_lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# returns multiple lines).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0marpa_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marpa_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marpa_to_ipa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marpa_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"source":["import itertools\n","\n","from gensim.models import Word2Vec\n","import numpy as np\n","import epitran\n","from tqdm import tqdm\n","\n","# ... (rest of the code remains the same) ...\n","\n","# Function to extract phonemes from a word using Epitran, handling unknown words\n","def get_phonemes(word):\n","    epi = epitran.Epitran('eng-Latn')\n","    try:\n","        # Attempt to transliterate the word\n","        phonemes = epi.transliterate(word)\n","        return phonemes, []  # Return phonemes and an empty list for unknown words\n","    except IndexError:\n","        # Handle the case where the word is not in Epitran's dictionary\n","        print(f\"Word '{word}' not found in Epitran's dictionary.\")\n","        return \"\", [word]  # Return empty string and the word in a list\n","\n","# Function to train Word2Vec on phoneme sequences, including a progress bar\n","def train_phoneme_embeddings(vocabulary):\n","    phoneme_sequences = []\n","    unknown_words = []\n","    for word in tqdm(vocabulary, desc=\"Generating phoneme sequences\"):\n","        phonemes, current_unknown_words = get_phonemes(word)\n","        if phonemes:  # If phonemes were successfully extracted\n","            phoneme_sequences.append(list(phonemes))\n","        unknown_words.extend(current_unknown_words) # Add any unknown words for this word to the overall list\n","\n","    model = Word2Vec(phoneme_sequences, vector_size=100, window=5, min_count=1, workers=4)\n","    return model, unknown_words  # Return the model and the list of unknown words\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = ['sea', 'see', 'pain', 'wine', 'key', 'sun', 'main', 'air', 'bear', 'belle', 'pie', 'cat', 'chat', 'father', 'mere']\n","phoneme_model, unknown_words = train_phoneme_embeddings(vocabulary) # Get the model and unknown words\n","embedding = phoneme_model.wv['t']\n","print(\"Embedding for 't':\", embedding)\n","\n","print(\"Unknown words:\", unknown_words) # Print the list of unknown words\n","\n","# ... (rest of the code remains the same) ..."],"cell_type":"code","metadata":{"id":"PuiWsAcvydNH"},"execution_count":null,"outputs":[]},{"source":["import itertools\n","\n","from gensim.models import Word2Vec\n","import numpy as np\n","import epitran\n","from tqdm import tqdm\n","\n","# ... (rest of the code remains the same) ...\n","\n","# Function to extract phonemes from a word using Epitran, handling unknown words\n","def get_phonemes(word):\n","    epi = epitran.Epitran('eng-Latn')\n","    try:\n","        # Attempt to transliterate the word\n","        phonemes = epi.transliterate(word)\n","        return phonemes, []  # Return phonemes and an empty list for unknown words\n","    except IndexError:\n","        # Handle the case where the word is not in Epitran's dictionary\n","        print(f\"Word '{word}' not found in Epitran's dictionary.\")\n","        return \"\", [word]  # Return empty string and the word in a list\n","\n","# Function to train Word2Vec on phoneme sequences, including a progress bar\n","def train_phoneme_embeddings(vocabulary):\n","    phoneme_sequences = []\n","    unknown_words = []\n","    for word in tqdm(vocabulary, desc=\"Generating phoneme sequences\"):\n","        phonemes, current_unknown_words = get_phonemes(word)\n","        if phonemes:  # If phonemes were successfully extracted\n","            phoneme_sequences.append(list(phonemes))\n","        unknown_words.extend(current_unknown_words) # Add any unknown words for this word to the overall list\n","\n","    model = Word2Vec(phoneme_sequences, vector_size=100, window=5, min_count=1, workers=4)\n","    return model, unknown_words  # Return the model and the list of unknown words\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = ['sea', 'see', 'pain', 'wine', 'key', 'sun', 'main', 'air', 'bear', 'belle', 'pie', 'cat', 'chat', 'father', 'mere']\n","phoneme_model, unknown_words = train_phoneme_embeddings(vocabulary) # Get the model and unknown words\n","embedding = phoneme_model.wv['t']\n","print(\"Embedding for 't':\", embedding)\n","\n","print(\"Unknown words:\", unknown_words) # Print the list of unknown words\n","\n","# ... (rest of the code remains the same) ..."],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mb_-dTkKydRd","executionInfo":{"status":"error","timestamp":1729006010762,"user_tz":-60,"elapsed":8807,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"abbfc49a-4077-4d41-ef5f-733c590b6483"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","Generating phoneme sequences:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[AWARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:   7%|▋         | 1/15 [00:00<00:08,  1.64it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'sea' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  13%|█▎        | 2/15 [00:01<00:07,  1.67it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'see' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  20%|██        | 3/15 [00:01<00:07,  1.67it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'pain' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  27%|██▋       | 4/15 [00:02<00:05,  1.85it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'wine' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  33%|███▎      | 5/15 [00:02<00:05,  1.81it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'key' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  40%|████      | 6/15 [00:03<00:05,  1.78it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'sun' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  47%|████▋     | 7/15 [00:03<00:04,  1.80it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'main' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  53%|█████▎    | 8/15 [00:04<00:03,  1.78it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'air' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  60%|██████    | 9/15 [00:05<00:03,  1.78it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'bear' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  67%|██████▋   | 10/15 [00:05<00:02,  1.77it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'belle' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  73%|███████▎  | 11/15 [00:06<00:02,  1.76it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'pie' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  80%|████████  | 12/15 [00:06<00:01,  1.74it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'cat' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  87%|████████▋ | 13/15 [00:07<00:01,  1.87it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'chat' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences:  93%|█████████▎| 14/15 [00:07<00:00,  1.81it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Word 'father' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:epitran:lex_lookup (from flite) is not installed.\n","\n","Generating phoneme sequences: 100%|██████████| 15/15 [00:08<00:00,  1.78it/s]"]},{"output_type":"stream","name":"stdout","text":["Word 'mere' not found in Epitran's dictionary.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"you must first build vocabulary before training the model","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-65371c16cd16>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Example usage of Word2Vec for phonemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sea'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'see'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'air'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'belle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pie'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'father'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mere'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mphoneme_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get the model and unknown words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoneme_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding for 't':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-65371c16cd16>\u001b[0m in \u001b[0;36mtrain_phoneme_embeddings\u001b[0;34m(vocabulary)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0munknown_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_unknown_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add any unknown words for this word to the overall list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneme_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_words\u001b[0m  \u001b[0;31m# Return the model and the list of unknown words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"]}]},{"cell_type":"code","source":["import itertools\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Load the phonetic dictionary from the uploaded CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['word'], df['phonetic']))\n","\n","# Define articulatory features for phonemes\n","articulatory_features = {\n","    't': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'ʔ': {'place': 'glottal', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'd': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiced'},\n","    's': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'z': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'ə': {'place': 'central', 'manner': 'vowel', 'voicing': 'voiced'},\n","    'ʃ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ʒ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'p': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'b': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiced'},\n","    'k': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'g': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiced'},\n","    'f': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'v': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'θ': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ð': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'm': {'place': 'bilabial', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'n': {'place': 'alveolar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'ŋ': {'place': 'velar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'h': {'place': 'glottal', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'j': {'place': 'palatal', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'w': {'place': 'labio-velar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'r': {'place': 'alveolar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'l': {'place': 'alveolar', 'manner': 'lateral approximant', 'voicing': 'voiced'},\n","    # Add more phonemes...\n","}\n","\n","# Phoneme mapping between similar sounds\n","phoneme_mapping = {\n","    'i': ['i', 'ɪ'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ'],\n","    'ɛ': ['ɛ', 'e'],\n","    'æ': ['a', 'æ'],\n","    'ɑ': ['a', 'ɑ'],\n","    'ɔ': ['o', 'ɔ'],\n","    'ə': ['ə', 'ʌ'],\n","    'ʌ': ['ə', 'ʌ'],\n","    't': ['t', 'ʔ', 'ɾ'],\n","    'd': ['t', 'ɾ'],\n","    's': ['s', 'z'],\n","    'z': ['s', 'z'],\n","    'ʃ': ['ʃ', 'ʒ'],\n","    'ʒ': ['ʃ', 'ʒ'],\n","    'n': ['n', 'ŋ'],\n","    'ŋ': ['n', 'ŋ'],\n","    'l': ['l', 'ɫ'],\n","    'r': ['r', 'ɹ'],\n","    'p': ['p', 'b'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g'],\n","    'g': ['k', 'g'],\n","    'f': ['f', 'v'],\n","    'v': ['f', 'v'],\n","    'θ': ['θ', 'ð'],\n","    'ð': ['θ', 'ð'],\n","    'h': ['h', 'ʔ'],\n","    'w': ['w', 'v'],\n","    # Add more based on phonetic similarities\n","}\n","\n","# Function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to merge phonemes with a gap represented as a glottal stop or ignoring the gap\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=False):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find homophones with gaps, including a progress bar and partial word matching\n","# Now includes printing matches in batches and using prosody tricks\n","def find_homophones_with_gaps(word_pairs, ignore_gap=False):\n","    homophones = []\n","    batch_size = 10\n","    for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding homophones with gaps\")):\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=ignore_gap)\n","        for other_word in vocabulary:\n","            other_phonemes = get_phonemes(other_word)\n","            # Check if merged phonemes match full or partially match\n","            if merged_phonemes == other_phonemes or merged_phonemes in other_phonemes or other_phonemes in merged_phonemes:\n","                homophones.append((f\"{word1} {word2}\", other_word))\n","        # Print matches in batches\n","        if (idx + 1) % batch_size == 0:\n","            print(f\"Matches for batch {idx // batch_size + 1}:\")\n","            for match in homophones[-batch_size:]:\n","                print(match)\n","    # Print final matches\n","    if len(homophones) % batch_size != 0:\n","        print(\"Final Matches:\")\n","        for match in homophones[-(len(homophones) % batch_size):]:\n","            print(match)\n","    return homophones\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence)\n","print(\"Phoneme Variants:\", sequences)\n","\n","# Function to use speech recognition and extract phonemes\n","def transcribe_audio_with_alternatives(audio_file):\n","    recognizer = sr.Recognizer()\n","    with sr.AudioFile(audio_file) as source:\n","        audio = recognizer.record(source)\n","    # Recognize with phoneme-level details (if supported)\n","    result = recognizer.recognize_google(audio, show_all=True)\n","    return result\n","\n","# Function to train Word2Vec on phoneme sequences, including a progress bar\n","def train_phoneme_embeddings(vocabulary):\n","    phoneme_sequences = [list(get_phonemes(word)) for word in tqdm(vocabulary, desc=\"Generating phoneme sequences\")]\n","    # Filter out words with no phonemes found\n","    phoneme_sequences = [seq for seq in phoneme_sequences if len(seq) > 0]\n","    model = Word2Vec(phoneme_sequences, vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = ['sea', 'see', 'pain', 'wine', 'key', 'sun', 'main', 'air', 'bear', 'belle', 'pie', 'cat', 'chat', 'father', 'mere']\n","phoneme_model = train_phoneme_embeddings(vocabulary)\n","embedding = phoneme_model.wv['t']\n","print(\"Embedding for 't':\", embedding)\n","\n","# Function to align two texts at the phoneme level\n","def align_texts_with_prosody(text1, text2):\n","    words1 = text1.split()\n","    words2 = text2.split()\n","    phonemes1 = [get_phonemes(word) for word in words1]\n","    phonemes2 = [get_phonemes(word) for word in words2]\n","    alignment = dynamic_phoneme_alignment(phonemes1, phonemes2)\n","    combined_text = generate_combined_text(alignment)\n","    return combined_text\n","\n","# Placeholder function for dynamic programming alignment\n","def dynamic_phoneme_alignment(seq1, seq2):\n","    # Implement dynamic programming alignment\n","    pass\n","\n","# Rule 1: Vowel Length Matching\n","def match_vowel_length(phoneme1, phoneme2):\n","    return phoneme1 in ['iː', 'i'] and phoneme2 in ['iː', 'i']\n","\n","# Rule 2: Aspiration and Fricative Interaction\n","def aspiration_and_fricative_interaction(phoneme1, phoneme2):\n","    return (phoneme1 == 'h' and phoneme2 == 'ʃ') or (phoneme1 == 'ʃ' and phoneme2 == 'h')\n","\n","# Rule 3: Schwa Neutralization\n","def schwa_neutralization(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 4: Glottal Stop Substitution\n","def glottal_stop_substitution(phoneme1, phoneme2):\n","    return (phoneme1 == 'ʔ' and phoneme2 == 't') or (phoneme1 == 't' and phoneme2 == 'ʔ')\n","\n","# Rule 5: Voicing Neutralization\n","def voicing_neutralization(phoneme1, phoneme2):\n","    voicing_pairs = [('s', 'z'), ('f', 'v'), ('θ', 'ð')]\n","    return (phoneme1, phoneme2) in voicing_pairs or (phoneme2, phoneme1) in voicing_pairs\n","\n","# Rule 6: Nasal Consonant Alternation\n","def nasal_consonant_alternation(phoneme1, phoneme2):\n","    return (phoneme1 == 'n' and phoneme2 == 'ŋ') or (phoneme1 == 'ŋ' and phoneme2 == 'n')\n","\n","# Rule 7: Alveolar Tap\n","def alveolar_tap(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 8: Vowel Reduction\n","def vowel_reduction(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 9: Place Assimilation\n","def place_assimilation(phoneme1, phoneme2, next_phoneme):\n","    return (phoneme1 == 'n' and next_phoneme == 'k' and phoneme2 == 'ŋ')\n","\n","# Rule 10: Lateral Consonant Variation\n","def lateral_consonant_variation(phoneme1, phoneme2):\n","    return (phoneme1 == 'l' and phoneme2 == 'ɫ') or (phoneme1 == 'ɫ' and phoneme2 == 'l')\n","\n","# Rule 11: Diphthong Simplification\n","def diphthong_simplification(phoneme1, phoneme2):\n","    return (phoneme1 == 'eɪ' and phoneme2 == 'e') or (phoneme1 == 'e' and phoneme2 == 'eɪ')\n","\n","# Rule 12: R-coloring\n","def r_coloring(phoneme1, phoneme2):\n","    return (phoneme1 == 'ɑr' and phoneme2 == 'ɝː') or (phoneme1 == 'ɝː' and phoneme2 == 'ɑr')\n","\n","# Rule 13: Voicing Assimilation\n","def voicing_assimilation(phoneme1, surrounding_phonemes):\n","    return phoneme1 in ['b', 'd', 'g'] and any(p in ['s', 't', 'k'] for p in surrounding_phonemes)\n","\n","# Rule 14: Glottal Replacement for Gaps\n","def glottal_replacement_for_gaps(phoneme1, phoneme2):\n","    return phoneme1 == 'ʔ' or phoneme2 == 'ʔ'\n","\n","# Rule 15: Deletion of Unstressed Syllables\n","def deletion_of_unstressed_syllables(word_phonemes):\n","    return [p for p in word_phonemes if p != 'ə']\n","\n","# Rule 16: Palatalization\n","def palatalization(phoneme1, next_phoneme):\n","    return phoneme1 == 't' and next_phoneme == 'j'\n","\n","# Rule 17: Flapping\n","def flapping(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 18: Coalescence of Alveolar and Palatal\n","def coalescence_alveolar_palatal(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ʃ') or (phoneme1 == 'd' and phoneme2 == 'ʒ')\n","\n","# Rule 19: Velar Fronting\n","def velar_fronting(phoneme1, next_phoneme):\n","    return phoneme1 == 'k' and next_phoneme in ['i', 'e']\n","\n","# Rule 20: Vowel Harmony\n","def vowel_harmony(phoneme1, phoneme2):\n","    return phoneme1 in ['i', 'e'] and phoneme2 in ['i', 'e']\n","\n","# Rule 21: Devoicing of Final Consonants\n","def devoicing_final_consonants(phoneme, position):\n","    return position == 'final' and phoneme in ['b', 'd', 'g']\n","\n","# Rule 22: Nasal Place Assimilation\n","def nasal_place_assimilation(phoneme1, next_phoneme):\n","    return phoneme1 == 'n' and next_phoneme in ['p', 'b', 'm']\n","\n","# Rule 23: Geminate Reduction\n","def geminate_reduction(phoneme_sequence):\n","    return [phoneme_sequence[i] for i in range(len(phoneme_sequence)) if i == 0 or phoneme_sequence[i] != phoneme_sequence[i-1]]\n","\n","# Rule 24: Consonant Cluster Simplification\n","def consonant_cluster_simplification(cluster):\n","    return [cluster[0]] if len(cluster) > 1 else cluster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":807},"id":"J8BJ7KoEzbFc","executionInfo":{"status":"error","timestamp":1729006297413,"user_tz":-60,"elapsed":988,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"69ca8fa2-32c6-47bc-831f-aaa90d4d0315"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rGenerating phoneme sequences:   0%|          | 0/15 [07:25<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyError","evalue":"'word'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'word'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-f57a3e53ef56>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the phonetic dictionary from the uploaded CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/en_UK.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mphonetic_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phonetic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Define articulatory features for phonemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'word'"]}]},{"cell_type":"code","source":["import itertools\n","\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Load the phonetic dictionary from the uploaded CSV file\n","# Assuming the delimiter is a tab ('\\t') and the file has a header row\n","df = pd.read_csv('/content/en_UK.txt', delimiter='\\t', header=0)\n","# If there's no header row, use header=None\n","\n","# If the column names are 'aah' and '/ˈɑː/' as in your example:\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))\n","# If the column names are different, adjust accordingly\n","\n","# Define articulatory features for phonemes\n","articulatory_features = {\n","    't': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'ʔ': {'place': 'glottal', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'd': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiced'},\n","    's': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'z': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'ə': {'place': 'central', 'manner': 'vowel', 'voicing': 'voiced'},\n","    'ʃ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ʒ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'p': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'b': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiced'},\n","    'k': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'g': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiced'},\n","    'f': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'v': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'θ': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ð': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'm': {'place': 'bilabial', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'n': {'place': 'alveolar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'ŋ': {'place': 'velar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'h': {'place': 'glottal', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'j': {'place': 'palatal', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'w': {'place': 'labio-velar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'r': {'place': 'alveolar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'l': {'place': 'alveolar', 'manner': 'lateral approximant', 'voicing': 'voiced'},\n","    # Add more phonemes...\n","}\n","\n","# Phoneme mapping between similar sounds\n","phoneme_mapping = {\n","    'i': ['i', 'ɪ'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ'],\n","    'ɛ': ['ɛ', 'e'],\n","    'æ': ['a', 'æ'],\n","    'ɑ': ['a', 'ɑ'],\n","    'ɔ': ['o', 'ɔ'],\n","    'ə': ['ə', 'ʌ'],\n","    'ʌ': ['ə', 'ʌ'],\n","    't': ['t', 'ʔ', 'ɾ'],\n","    'd': ['t', 'ɾ'],\n","    's': ['s', 'z'],\n","    'z': ['s', 'z'],\n","    'ʃ': ['ʃ', 'ʒ'],\n","    'ʒ': ['ʃ', 'ʒ'],\n","    'n': ['n', 'ŋ'],\n","    'ŋ': ['n', 'ŋ'],\n","    'l': ['l', 'ɫ'],\n","    'r': ['r', 'ɹ'],\n","    'p': ['p', 'b'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g'],\n","    'g': ['k', 'g'],\n","    'f': ['f', 'v'],\n","    'v': ['f', 'v'],\n","    'θ': ['θ', 'ð'],\n","    'ð': ['θ', 'ð'],\n","    'h': ['h', 'ʔ'],\n","    'w': ['w', 'v'],\n","    # Add more based on phonetic similarities\n","}\n","\n","# Function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find homophones with gaps, including a progress bar and partial word matching\n","# Now includes printing matches in batches and using prosody tricks\n","def find_homophones_with_gaps(word_pairs, ignore_gap=True):\n","    homophones = []\n","    batch_size = 10\n","    for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding homophones with gaps\")):\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=ignore_gap)\n","        for other_word in vocabulary:\n","            other_phonemes = get_phonemes(other_word)\n","            # Check if merged phonemes match full or partially match\n","            if merged_phonemes == other_phonemes or merged_phonemes in other_phonemes or other_phonemes in merged_phonemes:\n","                homophones.append((f\"{word1} {word2}\", other_word))\n","        # Print matches in batches\n","        if (idx + 1) % batch_size == 0:\n","            print(f\"Matches for batch {idx // batch_size + 1}:\")\n","            for match in homophones[-batch_size:]:\n","                print(match)\n","    # Print final matches\n","    if len(homophones) % batch_size != 0:\n","        print(\"Final Matches:\")\n","        for match in homophones[-(len(homophones) % batch_size):]:\n","            print(match)\n","    return homophones\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence)\n","print(\"Phoneme Variants:\", sequences)\n","\n","# Function to use speech recognition and extract phonemes\n","def transcribe_audio_with_alternatives(audio_file):\n","    recognizer = sr.Recognizer()\n","    with sr.AudioFile(audio_file) as source:\n","        audio = recognizer.record(source)\n","    # Recognize with phoneme-level details (if supported)\n","    result = recognizer.recognize_google(audio, show_all=True)\n","    return result\n","\n","# Function to train Word2Vec on phoneme sequences, including a progress bar\n","def train_phoneme_embeddings(vocabulary):\n","    phoneme_sequences = [list(get_phonemes(word)) for word in tqdm(vocabulary, desc=\"Generating phoneme sequences\")]\n","    # Filter out words with no phonemes found\n","    phoneme_sequences = [seq for seq in phoneme_sequences if len(seq) > 0]\n","    model = Word2Vec(phoneme_sequences, vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = ['sea', 'see', 'pain', 'wine', 'key', 'sun', 'main', 'air', 'bear', 'belle', 'pie', 'cat', 'chat', 'father', 'mere']\n","phoneme_model = train_phoneme_embeddings(vocabulary)\n","embedding = phoneme_model.wv['t']\n","print(\"Embedding for 't':\", embedding)\n","\n","# Function to align two texts at the phoneme level\n","def align_texts_with_prosody(text1, text2):\n","    words1 = text1.split()\n","    words2 = text2.split()\n","    phonemes1 = [get_phonemes(word) for word in words1]\n","    phonemes2 = [get_phonemes(word) for word in words2]\n","    alignment = dynamic_phoneme_alignment(phonemes1, phonemes2)\n","    combined_text = generate_combined_text(alignment)\n","    return combined_text\n","\n","# Placeholder function for dynamic programming alignment\n","def dynamic_phoneme_alignment(seq1, seq2):\n","    # Implement dynamic programming alignment\n","    pass\n","\n","# Rule 1: Vowel Length Matching\n","def match_vowel_length(phoneme1, phoneme2):\n","    return phoneme1 in ['iː', 'i'] and phoneme2 in ['iː', 'i']\n","\n","# Rule 2: Aspiration and Fricative Interaction\n","def aspiration_and_fricative_interaction(phoneme1, phoneme2):\n","    return (phoneme1 == 'h' and phoneme2 == 'ʃ') or (phoneme1 == 'ʃ' and phoneme2 == 'h')\n","\n","# Rule 3: Schwa Neutralization\n","def schwa_neutralization(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 4: Glottal Stop Substitution\n","def glottal_stop_substitution(phoneme1, phoneme2):\n","    return (phoneme1 == 'ʔ' and phoneme2 == 't') or (phoneme1 == 't' and phoneme2 == 'ʔ')\n","\n","# Rule 5: Voicing Neutralization\n","def voicing_neutralization(phoneme1, phoneme2):\n","    voicing_pairs = [('s', 'z'), ('f', 'v'), ('θ', 'ð')]\n","    return (phoneme1, phoneme2) in voicing_pairs or (phoneme2, phoneme1) in voicing_pairs\n","\n","# Rule 6: Nasal Consonant Alternation\n","def nasal_consonant_alternation(phoneme1, phoneme2):\n","    return (phoneme1 == 'n' and phoneme2 == 'ŋ') or (phoneme1 == 'ŋ' and phoneme2 == 'n')\n","\n","# Rule 7: Alveolar Tap\n","def alveolar_tap(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 8: Vowel Reduction\n","def vowel_reduction(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 9: Place Assimilation\n","def place_assimilation(phoneme1, phoneme2, next_phoneme):\n","    return (phoneme1 == 'n' and next_phoneme == 'k' and phoneme2 == 'ŋ')\n","\n","# Rule 10: Lateral Consonant Variation\n","def lateral_consonant_variation(phoneme1, phoneme2):\n","    return (phoneme1 == 'l' and phoneme2 == 'ɫ') or (phoneme1 == 'ɫ' and phoneme2 == 'l')\n","\n","# Rule 11: Diphthong Simplification\n","def diphthong_simplification(phoneme1, phoneme2):\n","    return (phoneme1 == 'eɪ' and phoneme2 == 'e') or (phoneme1 == 'e' and phoneme2 == 'eɪ')\n","\n","# Rule 12: R-coloring\n","def r_coloring(phoneme1, phoneme2):\n","    return (phoneme1 == 'ɑr' and phoneme2 == 'ɝː') or (phoneme1 == 'ɝː' and phoneme2 == 'ɑr')\n","\n","# Rule 13: Voicing Assimilation\n","def voicing_assimilation(phoneme1, surrounding_phonemes):\n","    return phoneme1 in ['b', 'd', 'g'] and any(p in ['s', 't', 'k'] for p in surrounding_phonemes)\n","\n","# Rule 14: Glottal Replacement for Gaps\n","def glottal_replacement_for_gaps(phoneme1, phoneme2):\n","    return phoneme1 == 'ʔ' or phoneme2 == 'ʔ'\n","\n","# Rule 15: Deletion of Unstressed Syllables\n","def deletion_of_unstressed_syllables(word_phonemes):\n","    return [p for p in word_phonemes if p != 'ə']\n","\n","# Rule 16: Palatalization\n","def palatalization(phoneme1, next_phoneme):\n","    return phoneme1 == 't' and next_phoneme == 'j'\n","\n","# Rule 17: Flapping\n","def flapping(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 18: Coalescence of Alveolar and Palatal\n","def coalescence_alveolar_palatal(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ʃ') or (phoneme1 == 'd' and phoneme2 == 'ʒ')\n","\n","# Rule 19: Velar Fronting\n","def velar_fronting(phoneme1, next_phoneme):\n","    return phoneme1 == 'k' and next_phoneme in ['i', 'e']\n","\n","# Rule 20: Vowel Harmony\n","def vowel_harmony(phoneme1, phoneme2):\n","    return phoneme1 in ['i', 'e'] and phoneme2 in ['i', 'e']\n","\n","# Rule 21: Devoicing of Final Consonants\n","def devoicing_final_consonants(phoneme, position):\n","    return position == 'final' and phoneme in ['b', 'd', 'g']\n","\n","# Rule 22: Nasal Place Assimilation\n","def nasal_place_assimilation(phoneme1, next_phoneme):\n","    return phoneme1 == 'n' and next_phoneme in ['p', 'b', 'm']\n","\n","# Rule 23: Geminate Reduction\n","def geminate_reduction(phoneme_sequence):\n","    return [phoneme_sequence[i] for i in range(len(phoneme_sequence)) if i == 0 or phoneme_sequence[i] != phoneme_sequence[i-1]]\n","\n","# Rule 24: Consonant Cluster Simplification\n","def consonant_cluster_simplification(cluster):\n","    return [cluster[0]] if len(cluster) > 1 else cluster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hC_G4w0X0wXX","executionInfo":{"status":"ok","timestamp":1737895915437,"user_tz":0,"elapsed":207,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"4d8434c4-9d80-48d8-8d7e-ec421c9fcfd5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Phoneme Variants: ['pətər', 'pəʔər', 'pəɾər', 'pʌtər', 'pʌʔər', 'pʌɾər', 'bətər', 'bəʔər', 'bəɾər', 'bʌtər', 'bʌʔər', 'bʌɾər']\n"]},{"output_type":"stream","name":"stderr","text":["Generating phoneme sequences: 100%|██████████| 15/15 [00:00<00:00, 133011.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Embedding for 't': [-0.00512888 -0.00665975 -0.00777412  0.00829222 -0.00196878 -0.00685943\n"," -0.0041453   0.0051871  -0.00290524 -0.00377401  0.00161805 -0.00280275\n"," -0.00156002  0.00108358 -0.00296945  0.00849498  0.00395642 -0.00991243\n","  0.00624722 -0.0068219   0.00074653  0.00438835 -0.00510291 -0.0021328\n","  0.00809312 -0.0042437  -0.00764467  0.00926354 -0.00215973 -0.00470555\n","  0.00859486  0.00428176  0.0043055   0.00928365 -0.00848831  0.00526324\n","  0.00203329  0.00417708  0.00168644  0.00444292  0.00450615  0.00608009\n"," -0.00318307 -0.00455266 -0.00041465  0.0025331  -0.00328302  0.00601064\n","  0.00414005  0.00776056  0.00258358  0.00808698 -0.00141263  0.00806025\n","  0.00367884 -0.0080712  -0.00393636 -0.00249315  0.00488203 -0.00087672\n"," -0.00282813  0.00780824  0.00937821 -0.00159107 -0.00518596 -0.00464311\n"," -0.00483983 -0.0095965   0.00133052 -0.00423238  0.00253992  0.00564295\n"," -0.00404601 -0.00957192  0.00157322 -0.00666936  0.00254506 -0.00375458\n","  0.00706008  0.00066126  0.00356153 -0.00274978 -0.00174111  0.00768012\n","  0.00140465 -0.00586575 -0.00781304  0.00122109  0.00648424  0.00557115\n"," -0.0089684   0.00860986  0.00406956  0.00746372  0.00978385 -0.00728228\n"," -0.00898924  0.00582533  0.00942475  0.00349746]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import itertools\n","\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from difflib import SequenceMatcher\n","import csv\n","\n","# Load the phonetic dictionary from the uploaded CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define articulatory features for phonemes\n","articulatory_features = {\n","    't': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'ʔ': {'place': 'glottal', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'd': {'place': 'alveolar', 'manner': 'stop', 'voicing': 'voiced'},\n","    's': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'z': {'place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'ə': {'place': 'central', 'manner': 'vowel', 'voicing': 'voiced'},\n","    'ʃ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ʒ': {'place': 'postalveolar', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'p': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'b': {'place': 'bilabial', 'manner': 'stop', 'voicing': 'voiced'},\n","    'k': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiceless'},\n","    'g': {'place': 'velar', 'manner': 'stop', 'voicing': 'voiced'},\n","    'f': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'v': {'place': 'labiodental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'θ': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'ð': {'place': 'dental', 'manner': 'fricative', 'voicing': 'voiced'},\n","    'm': {'place': 'bilabial', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'n': {'place': 'alveolar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'ŋ': {'place': 'velar', 'manner': 'nasal', 'voicing': 'voiced'},\n","    'h': {'place': 'glottal', 'manner': 'fricative', 'voicing': 'voiceless'},\n","    'j': {'place': 'palatal', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'w': {'place': 'labio-velar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'r': {'place': 'alveolar', 'manner': 'approximant', 'voicing': 'voiced'},\n","    'l': {'place': 'alveolar', 'manner': 'lateral approximant', 'voicing': 'voiced'},\n","    # Add more phonemes...\n","}\n","\n","# Phoneme mapping between similar sounds\n","phoneme_mapping = {\n","    'i': ['i', 'ɪ'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ'],\n","    'ɛ': ['ɛ', 'e'],\n","    'æ': ['a', 'æ'],\n","    'ɑ': ['a', 'ɑ'],\n","    'ɔ': ['o', 'ɔ'],\n","    'ə': ['ə', 'ʌ'],\n","    'ʌ': ['ə', 'ʌ'],\n","    't': ['t', 'ʔ', 'ɾ'],\n","    'd': ['t', 'ɾ'],\n","    's': ['s', 'z'],\n","    'z': ['s', 'z'],\n","    'ʃ': ['ʃ', 'ʒ'],\n","    'ʒ': ['ʃ', 'ʒ'],\n","    'n': ['n', 'ŋ'],\n","    'ŋ': ['n', 'ŋ'],\n","    'l': ['l', 'ɫ'],\n","    'r': ['r', 'ɹ'],\n","    'p': ['p', 'b'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g'],\n","    'g': ['k', 'g'],\n","    'f': ['f', 'v'],\n","    'v': ['f', 'v'],\n","    'θ': ['θ', 'ð'],\n","    'ð': ['θ', 'ð'],\n","    'h': ['h', 'ʔ'],\n","    'w': ['w', 'v'],\n","    # Add more based on phonetic similarities\n","}\n","\n","# Function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to find homophones with gaps, including a progress bar, partial word matching, and similarity metric\n","# Now includes printing matches in batches and using prosody tricks\n","def find_homophones_with_gaps(word_pairs, ignore_gap=True, similarity_threshold=0.8):\n","    homophones = []\n","    batch_size = 10\n","    with open('homophone_matches.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word Pair', 'Matched Word', 'Similarity'])\n","        for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding homophones with gaps\")):\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=ignore_gap)\n","            for other_word in phonetic_dict.keys():  # Iterate over the entire phonetic dictionary\n","                other_phonemes = get_phonemes(other_word)\n","                similarity = phoneme_similarity(merged_phonemes, other_phonemes)\n","                if similarity >= similarity_threshold:\n","                    homophones.append((f\"{word1} {word2}\", other_word, similarity))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, similarity])\n","            # Print matches in batches\n","            if (idx + 1) % batch_size == 0:\n","                print(f\"Matches for batch {idx // batch_size + 1}:\")\n","                for match in homophones[-batch_size:]:\n","                    print(match)\n","        # Print final matches\n","        if len(homophones) % batch_size != 0:\n","            print(\"Final Matches:\")\n","            for match in homophones[-(len(homophones) % batch_size):]:\n","                print(match)\n","    return homophones\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence)\n","print(\"Phoneme Variants:\", sequences)\n","\n","# Function to train Word2Vec on phoneme sequences, including a progress bar\n","def train_phoneme_embeddings(vocabulary):\n","    phoneme_sequences = [list(get_phonemes(word)) for word in tqdm(vocabulary, desc=\"Generating phoneme sequences\")]\n","    # Filter out words with no phonemes found\n","    phoneme_sequences = [seq for seq in phoneme_sequences if len(seq) > 0]\n","    model = Word2Vec(phoneme_sequences, vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = list(phonetic_dict.keys())\n","phoneme_model = train_phoneme_embeddings(vocabulary)\n","embedding = phoneme_model.wv['t']\n","print(\"Embedding for 't':\", embedding)\n","\n","# Function to align two texts at the phoneme level\n","def align_texts_with_prosody(text1, text2):\n","    words1 = text1.split()\n","    words2 = text2.split()\n","    phonemes1 = [get_phonemes(word) for word in words1]\n","    phonemes2 = [get_phonemes(word) for word in words2]\n","    alignment = dynamic_phoneme_alignment(phonemes1, phonemes2)\n","    combined_text = generate_combined_text(alignment)\n","    return combined_text\n","\n","# Placeholder function for dynamic programming alignment\n","def dynamic_phoneme_alignment(seq1, seq2):\n","    # Implement dynamic programming alignment\n","    pass\n","\n","# Rule 1: Vowel Length Matching\n","def match_vowel_length(phoneme1, phoneme2):\n","    return phoneme1 in ['iː', 'i'] and phoneme2 in ['iː', 'i']\n","\n","# Rule 2: Aspiration and Fricative Interaction\n","def aspiration_and_fricative_interaction(phoneme1, phoneme2):\n","    return (phoneme1 == 'h' and phoneme2 == 'ʃ') or (phoneme1 == 'ʃ' and phoneme2 == 'h')\n","\n","# Rule 3: Schwa Neutralization\n","def schwa_neutralization(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 4: Glottal Stop Substitution\n","def glottal_stop_substitution(phoneme1, phoneme2):\n","    return (phoneme1 == 'ʔ' and phoneme2 == 't') or (phoneme1 == 't' and phoneme2 == 'ʔ')\n","\n","# Rule 5: Voicing Neutralization\n","def voicing_neutralization(phoneme1, phoneme2):\n","    voicing_pairs = [('s', 'z'), ('f', 'v'), ('θ', 'ð')]\n","    return (phoneme1, phoneme2) in voicing_pairs or (phoneme2, phoneme1) in voicing_pairs\n","\n","# Rule 6: Nasal Consonant Alternation\n","def nasal_consonant_alternation(phoneme1, phoneme2):\n","    return (phoneme1 == 'n' and phoneme2 == 'ŋ') or (phoneme1 == 'ŋ' and phoneme2 == 'n')\n","\n","# Rule 7: Alveolar Tap\n","def alveolar_tap(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 8: Vowel Reduction\n","def vowel_reduction(phoneme1, phoneme2):\n","    return phoneme1 == 'ə' or phoneme2 == 'ə'\n","\n","# Rule 9: Place Assimilation\n","def place_assimilation(phoneme1, phoneme2, next_phoneme):\n","    return (phoneme1 == 'n' and next_phoneme == 'k' and phoneme2 == 'ŋ')\n","\n","# Rule 10: Lateral Consonant Variation\n","def lateral_consonant_variation(phoneme1, phoneme2):\n","    return (phoneme1 == 'l' and phoneme2 == 'ɫ') or (phoneme1 == 'ɫ' and phoneme2 == 'l')\n","\n","# Rule 11: Diphthong Simplification\n","def diphthong_simplification(phoneme1, phoneme2):\n","    return (phoneme1 == 'eɪ' and phoneme2 == 'e') or (phoneme1 == 'e' and phoneme2 == 'eɪ')\n","\n","# Rule 12: R-coloring\n","def r_coloring(phoneme1, phoneme2):\n","    return (phoneme1 == 'ɑr' and phoneme2 == 'ɝː') or (phoneme1 == 'ɝː' and phoneme2 == 'ɑr')\n","\n","# Rule 13: Voicing Assimilation\n","def voicing_assimilation(phoneme1, surrounding_phonemes):\n","    return phoneme1 in ['b', 'd', 'g'] and any(p in ['s', 't', 'k'] for p in surrounding_phonemes)\n","\n","# Rule 14: Glottal Replacement for Gaps\n","def glottal_replacement_for_gaps(phoneme1, phoneme2):\n","    return phoneme1 == 'ʔ' or phoneme2 == 'ʔ'\n","\n","# Rule 15: Deletion of Unstressed Syllables\n","def deletion_of_unstressed_syllables(word_phonemes):\n","    return [p for p in word_phonemes if p != 'ə']\n","\n","# Rule 16: Palatalization\n","def palatalization(phoneme1, next_phoneme):\n","    return phoneme1 == 't' and next_phoneme == 'j'\n","\n","# Rule 17: Flapping\n","def flapping(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ɾ') or (phoneme1 == 'd' and phoneme2 == 'ɾ')\n","\n","# Rule 18: Coalescence of Alveolar and Palatal\n","def coalescence_alveolar_palatal(phoneme1, phoneme2):\n","    return (phoneme1 == 't' and phoneme2 == 'ʃ') or (phoneme1 == 'd' and phoneme2 == 'ʒ')\n","\n","# Rule 19: Velar Fronting\n","def velar_fronting(phoneme1, next_phoneme):\n","    return phoneme1 == 'k' and next_phoneme in ['i', 'e']\n","\n","# Rule 20: Vowel Harmony\n","def vowel_harmony(phoneme1, phoneme2):\n","    return phoneme1 in ['i', 'e'] and phoneme2 in ['i', 'e']\n","\n","# Rule 21: Devoicing of Final Consonants\n","def devoicing_final_consonants(phoneme, position):\n","    return position == 'final' and phoneme in ['b', 'd', 'g']\n","\n","# Rule 22: Nasal Place Assimilation\n","def nasal_place_assimilation(phoneme1, next_phoneme):\n","    return phoneme1 == 'n' and next_phoneme in ['p', 'b', 'm']\n","\n","# Rule 23: Geminate Reduction\n","def geminate_reduction(phoneme_sequence):\n","    return [phoneme_sequence[i] for i in range(len(phoneme_sequence)) if i == 0 or phoneme_sequence[i] != phoneme_sequence[i-1]]\n","\n","# Rule 24: Consonant Cluster Simplification\n","def consonant_cluster_simplification(cluster):\n","    return [cluster[0]] if len(cluster) > 1 else cluster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":688},"id":"e1L4nDlb4ftx","executionInfo":{"status":"error","timestamp":1729007636930,"user_tz":-60,"elapsed":1591,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"873e2187-de06-4d63-fe34-f0416d18cfe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Phoneme Variants: ['pətər', 'pəʔər', 'pəɾər', 'pʌtər', 'pʌʔər', 'pʌɾər', 'bətər', 'bəʔər', 'bəɾər', 'bʌtər', 'bʌʔər', 'bʌɾər']\n"]},{"output_type":"stream","name":"stderr","text":["Generating phoneme sequences:  15%|█▌        | 10045/65116 [00:00<00:00, 74478.97it/s]"]},{"output_type":"error","ename":"AttributeError","evalue":"'float' object has no attribute 'lower'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-452f6ff3ea9f>\u001b[0m in \u001b[0;36m<cell line: 147>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# Example usage of Word2Vec for phonemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphonetic_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mphoneme_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoneme_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding for 't':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-452f6ff3ea9f>\u001b[0m in \u001b[0;36mtrain_phoneme_embeddings\u001b[0;34m(vocabulary)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Function to train Word2Vec on phoneme sequences, including a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mphoneme_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating phoneme sequences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Filter out words with no phonemes found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mphoneme_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphoneme_sequences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-452f6ff3ea9f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Function to train Word2Vec on phoneme sequences, including a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_phoneme_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mphoneme_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating phoneme sequences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Filter out words with no phonemes found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mphoneme_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphoneme_sequences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-452f6ff3ea9f>\u001b[0m in \u001b[0;36mget_phonemes\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Function to extract phonemes from a word using the phonetic dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mphonetic_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Function to merge phonemes ignoring gaps for matching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"]}]},{"source":["import re\n","import itertools\n","\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Load the phonetic dictionary from the uploaded CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","\n","# Filter rows where 'Word' starts with 'A' followed by a number\n","pattern = r'^A\\d+'  # Regular expression pattern for 'A' followed by one or more digits\n","filtered_df = df[df['Word'].str.match(pattern, na=False)]\n","\n","# Create the phonetic dictionary\n","phonetic_dict = dict(zip(filtered_df['Word'], filtered_df['Phoneme']))\n","\n","# ... (Rest of the code remains the same)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"id":"URJ-RFxU2Phv","executionInfo":{"status":"error","timestamp":1729006995073,"user_tz":-60,"elapsed":404,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"f607ac7c-1f15-408d-c5c1-e193fe5cea5e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'Word'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Word'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-3aeb2550234e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Filter rows where 'Word' starts with 'A' followed by a number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'^A\\d+'\u001b[0m  \u001b[0;31m# Regular expression pattern for 'A' followed by one or more digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Create the phonetic dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Word'"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","\n","# Display the column names to see if they match what you expect\n","print(df.columns)\n","\n","# Show a preview of the data to verify the structure\n","print(df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rGBEfJA2wwt","executionInfo":{"status":"ok","timestamp":1729007140845,"user_tz":-60,"elapsed":737,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"040e5a8a-cd56-49b2-f7ec-b1133d739b6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['aah', '/ˈɑː/'], dtype='object')\n","         aah        /ˈɑː/\n","0   aardvark   /ˈɑːdvɑːk/\n","1  aardvarks  /ˈɑːdvɑːks/\n","2   aardwolf   /ˈɑːdwʊlf/\n","3        aba       /ɐbˈæ/\n","4      abaca     /æbˈækɐ/\n"]}]},{"cell_type":"code","source":["# Reference file for phoneme mapping\n","\n","# Consolidated phoneme mapping with advanced linguistic rules\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],                          # High front vowels, long/short interchange\n","    'ɪ': ['i', 'ɪ'],                                 # Close front vowel, similar in many accents\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],                    # Diphthong mapping to monophthongs or other diphthongs\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],                      # Mid front, flexible in accents (e.g., Northern English \"e\" vs \"æ\")\n","    'æ': ['a', 'æ', 'ɑ'],                             # Low front, often replaced by \"a\" or \"ɑ\" in regional accents\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],                        # Open back, overlaps with \"a\" and rounded in some dialects\n","    'ɔ': ['o', 'ɔ', 'ɒ'],                             # Mid back, can interchange with \"o\" or low back rounded vowel\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],                        # Schwa, interchangeable in unstressed positions\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],                             # Open-mid back, often overlaps with schwa\n","    'u': ['u', 'ʊ'],                                  # High back, interchanges with rounded versions\n","    'ʊ': ['u', 'ʊ', 'oʊ'],                            # Close back rounded, also maps with certain diphthongs\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],                             # Mid central vowel, similar to schwa or rhotic version\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],                        # Back rounded, can vary regionally to open front\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],                       # Alveolar stop, changes to glottal stop, flap, or labialized variant\n","    'd': ['t', 'ɾ', 'd'],                             # Alveolar stop, interchange with voiced or voiceless versions\n","    's': ['s', 'z', 'θ'],                             # Alveolar fricative, voicing or assimilation to dental\n","    'z': ['s', 'z', 'ð'],                             # Voiced fricative, voiceless equivalent or dental substitution\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],                            # Postalveolar fricative, can become affricate or voiced version\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],                            # Voiced version, matches affricate and fricative pairs\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],                        # Nasals flexible depending on position or assimilation\n","    'ŋ': ['n', 'ŋ', 'ɲ'],                             # Velar nasal, interchangeable in word-final or medial\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],                        # Lateral approximants, dark \"l\" (syllable-final)\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],               # Rhotics, dropped or replaced with schwa in some dialects\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],                       # Voiceless bilabial stop, can become glottal or aspirated\n","    'b': ['p', 'b'],                                  # Voiced bilabial, softens or voices to pair\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],                       # Velar stop, can become glottal or aspirated\n","    'g': ['k', 'g'],                                  # Voiced velar, interchangeable with unvoiced equivalent\n","    'h': ['', 'h', 'ʔ'],                              # H-dropping and glottal substitution\n","    'f': ['f', 'v', 'θ'],                             # Fricative voicing lenition or substitution with dental\n","    'θ': ['θ', 'f', 't'],                             # Th-fronting or substitution with \"t\" in certain accents\n","    'ð': ['ð', 'v', 'd'],                             # Voiced equivalent, voicing changes or replaced by stop\n","    'j': ['', 'j', 'tʃ', 'dʒ'],                       # Yod-coalescence, yod-dropping, or affricate formation\n","    'w': ['w', 'ʍ'],                                  # Labialized velar or voiceless version (wh-sound)\n","    'v': ['v', 'f', 'w'],                             # Labio-dental voiced to voiceless substitution\n","    'm': ['m', 'n'],                                  # Labial to alveolar nasal in connected speech\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],                                   # Can be a gap or silent in blending two words\n","    'ɾ': ['t', 'd', 'ɾ'],                             # Flap, typical in American English (e.g., butter)\n","    'ʊ': ['ʊ', 'ɫ'],                                  # Dark l as final sound alternative (syllabic l)\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],                                # Aspirated or unaspirated based on context\n","    'tʰ': ['t', 'tʰ'],                                # Alveolar, aspirated/unaspirated\n","    'kʰ': ['k', 'kʰ'],                                # Velar, aspirated/unaspirated\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],                    # Diphthong reduced to monophthong in some accents\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],                           # Diphthong reduction or smoothening\n","    'aʊ': ['aʊ', 'æ', 'o'],                           # Dipthong reduced based on accent or emphasis\n","    'eɪ': ['eɪ', 'e', 'ɛ'],                           # Reduction or merging with other vowels\n","    'eə': ['eə', 'ɜ', 'ə'],                           # Reduction to schwa or smoothened articulation\n","    'ʌɪ': ['aɪ', 'aː'],                               # Similar in diphthongs, common in dialect shift\n","\n","    # Reduction to Schwa and Stress Variability\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],                        # Schwa replaced with context-specific vowels\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],                            # Stressed/unstressed variations based on regional stress\n","\n","    # Lateralization and Coarticulation\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],                          # Affricate reduced to fricative or voiced counterpart\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],                          # Affricate voiced/voiceless interplay\n","    'tw': ['t', 'tw'],                                # Labialization and added rounding (e.g., \"twenty\")\n","}\n","\n","### Additional Linguistic Features & Considerations\n","\n","rules = {\n","    \"aspiration_lenition_fortition\": lambda p1, surrounding: p1 in ['p', 't', 'k'] and surrounding in ['pʰ', 'tʰ', 'kʰ'],\n","    \"glottal_stops_for_gaps\": lambda p1, p2: (p1 == '' or p2 == 'ʔ'),\n","    \"intrusive_linking_r\": lambda p1, p2: p1 == 'r' or p2 == 'r',\n","    \"th_fronting\": lambda p1, p2: p1 == 'θ' and p2 in ['f', 't'],\n","    \"dark_l_vocalized_l\": lambda p1, position: p1 == 'l' and position == 'syllable-final',\n","    \"vowel_reduction_schwa\": lambda p1, position: p1 in ['a', 'e', 'o', 'u'] and position == 'unstressed',\n","    \"nasal_place_assimilation\": lambda p1, next_p: p1 == 'n' and next_p in ['p', 'b', 'm'],\n","    \"flapping\": lambda p1, p2: p1 in ['t', 'd'] and p2 == 'ɾ',\n","    \"coarticulation_effects\": lambda p1, context: p1 in ['h'] and context == 'casual',\n","    \"h_dropping\": lambda p1, position: p1 == 'h' and position == 'unstressed',\n","    \"nasal_g_dropping\": lambda p1, position: p1 == 'ŋ' and position == 'word-final',\n","   \"affricate_fricative_coalescence\": lambda p1, p2: (p1 == 'tʃ' and p2 == 'ʃ') or (p1 == 'dʒ' and p2 == 'ʒ'),\n","    \"prosodic_markers_stress_patterns\": lambda stress: stress in ['primary', 'secondary'],  # Recognizing stress levels\n","    \"glottal_reinforcement\": lambda p1, stress: p1 == 'ʔ' and stress == 'stressed',\n","    \"meter_tempo_accentuation\": lambda context: context in ['poetry', 'lyrical'],  # Applying rhythmic modifications\n","    \"linking_r\": lambda p1, p2: p1 == 'r' or p2 == 'r',  # Linking R in non-rhotic accents\n","    \"vowel_harmony\": lambda p1, p2: p1 in ['i', 'e'] and p2 in ['i', 'e'],  # Vowel harmonization\n","     \"devoicing_final_consonants\": lambda p1, position: p1 in ['b', 'd', 'g'] and position == 'final',  # Devoicing at end of word\n","}"],"metadata":{"id":"8HMoLAq39RC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["```python\n","# Reference file for phoneme mapping\n","\n","# Import phoneme mapping from the reference file\n","# from /content/phoneme_mapping_reference_py.py import /content/phoneme_mapping_reference_py.py\n","import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find matches considering phoneme mapping, partial word matching, multi-word matches, and writing detailed results to a CSV file\n","def find_matches(word_pairs, similarity_threshold=0.8):\n","    matches = []\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","        for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding matches\")):\n","            phoneme_seq1 = get_phonemes(word1)\n","            phoneme_seq2 = get_phonemes(word2)\n","            similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","            match_type = \"\"\n","\n","            # Full word match\n","            if similarity >= similarity_threshold:\n","                match_type = \"Full word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Partial word matches\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Multi-word matches by merging phonemes\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","            for other_word in phonetic_dict.keys():\n","                other_phonemes = get_phonemes(other_word)\n","                if merged_phonemes == other_phonemes:\n","                    match_type = \"Multi-word match\"\n","                    matches.append((f\"{word1} {word2}\", other_word, similarity, match_type))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, similarity, match_type])\n","                    break\n","\n","    return matches\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence, phoneme_mapping):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_mapping = {'t': ['d', 't']} # Example mapping\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence, phoneme_mapping)\n","# Import phoneme mapping from the reference file\n","# from /content/phoneme_mapping_reference_py.py import /content/phoneme_mapping_reference_py.py\n","import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find matches considering phoneme mapping, partial word matching, multi-word matches, and writing detailed results to a CSV file\n","def find_matches(word_pairs, similarity_threshold=0.8):\n","    matches = []\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","        for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding matches\")):\n","            phoneme_seq1 = get_phonemes(word1)\n","            phoneme_seq2 = get_phonemes(word2)\n","            similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","            match_type = \"\"\n","\n","            # Full word match\n","            if similarity >= similarity_threshold:\n","                match_type = \"Full word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Partial word matches\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Multi-word matches by merging phonemes\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","            for other_word in phonetic_dict.keys():\n","                other_phonemes = get_phonemes(other_word)\n","                if merged_phonemes == other_phonemes:\n","                    match_type = \"Multi-word match\"\n","                    matches.append((f\"{word1} {word2}\", other_word, similarity, match_type))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, similarity, match_type])\n","                    break\n","\n","    return matches\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","phoneme_mapping = {'t': ['d', 't']}  # Example mapping\n","\n","# Example usage of equivalencies\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence)\n","print(\"Phoneme Variants:\", sequences)\n","# Import phoneme mapping from the reference file\n","# from /content/phoneme_mapping_reference_py.py import /content/phoneme_mapping_reference_py.py\n","import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find matches considering phoneme mapping, partial word matching, multi-word matches, and writing detailed results to a CSV file\n","def find_matches(word_pairs, similarity_threshold=0.8):\n","    matches = []\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","        for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding matches\")):\n","            phoneme_seq1 = get_phonemes(word1)\n","            phoneme_seq2 = get_phonemes(word2)\n","            similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","            match_type = \"\"\n","\n","            # Full word match\n","            if similarity >= similarity_threshold:\n","                match_type = \"Full word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Partial word matches\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Multi-word matches by merging phonemes\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","            for other_word in phonetic_dict.keys():\n","                other_phonemes = get_phonemes(other_word)\n","                if merged_phonemes == other_phonemes:\n","                    match_type = \"Multi-word match\"\n","                    matches.append((f\"{word1} {word2}\", other_word, similarity, match_type))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, similarity, match_type])\n","                    break\n","\n","    return matches\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence, phoneme_mapping):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_mapping = {'t': ['d', 't']} # Example mapping\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence, phoneme_mapping)\n","from tqdm import tqdm\n","import csv\n","import itertools\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find matches considering phoneme mapping, partial word matching, multi-word matches, and writing detailed results to a CSV file\n","def find_matches(word_pairs, similarity_threshold=0.8):\n","    matches = []\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","        for idx, (word1, word2) in enumerate(tqdm(word_pairs, desc=\"Finding matches\")):\n","            phoneme_seq1 = get_phonemes(word1)\n","            phoneme_seq2 = get_phonemes(word2)\n","            similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","            match_type = \"\"\n","\n","            # Full word match\n","            if similarity >= similarity_threshold:\n","                match_type = \"Full word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Partial word matches\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Multi-word matches by merging phonemes\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","            for other_word in phonetic_dict.keys():\n","                other_phonemes = get_phonemes(other_word)\n","                if merged_phonemes == other_phonemes:\n","                    match_type = \"Multi-word match\"\n","                    matches.append((f\"{word1} {word2}\", other_word, similarity, match_type))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, similarity, match_type])\n","                    break\n","\n","    return matches\n","\n","# Function to apply equivalencies to generate phoneme variants\n","def apply_equivalencies(phoneme_sequence):\n","    phoneme_options = []\n","    for phoneme in phoneme_sequence:\n","        options = phoneme_mapping.get(phoneme, [phoneme])\n","        phoneme_options.append(options)\n","    possible_sequences = [''.join(seq) for seq in itertools.product(*phoneme_options)]\n","    return possible_sequences\n","\n","# Example usage of equivalencies\n","phoneme_sequence = ['b', 'ʌ', 't', 'ər']\n","sequences = apply_equivalencies(phoneme_sequence)\n","print(\"Phoneme Variants:\", sequences)\n","\n","# Example usage of Word2Vec for phonemes\n","vocabulary = list(phonetic_dict.keys())\n","word_pairs = [(word1, word2) for word1 in vocabulary for word2 in vocabulary if word1 != word2]\n","find_matches(word_pairs, similarity_threshold=0.8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"Hg2CsQIHAhAY","executionInfo":{"status":"error","timestamp":1729010223936,"user_tz":-60,"elapsed":352,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"a90981d6-29e3-4bd2-8219-72d2616ce45e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-38-24676bf79268>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-24676bf79268>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["# Reference file for phoneme mapping\n","\n","# Consolidated phoneme mapping with advanced linguistic rules\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],                          # High front vowels, long/short interchange\n","    'ɪ': ['i', 'ɪ'],                                 # Close front vowel, similar in many accents\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],                    # Diphthong mapping to monophthongs or other diphthongs\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],                      # Mid front, flexible in accents (e.g., Northern English \"e\" vs \"æ\")\n","    'æ': ['a', 'æ', 'ɑ'],                             # Low front, often replaced by \"a\" or \"ɑ\" in regional accents\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],                        # Open back, overlaps with \"a\" and rounded in some dialects\n","    'ɔ': ['o', 'ɔ', 'ɒ'],                             # Mid back, can interchange with \"o\" or low back rounded vowel\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],                        # Schwa, interchangeable in unstressed positions\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],                             # Open-mid back, often overlaps with schwa\n","    'u': ['u', 'ʊ'],                                  # High back, interchanges with rounded versions\n","    'ʊ': ['u', 'ʊ', 'oʊ'],                            # Close back rounded, also maps with certain diphthongs\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],                             # Mid central vowel, similar to schwa or rhotic version\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],                        # Back rounded, can vary regionally to open front\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],                       # Alveolar stop, changes to glottal stop, flap, or labialized variant\n","    'd': ['t', 'ɾ', 'd'],                             # Alveolar stop, interchange with voiced or voiceless versions\n","    's': ['s', 'z', 'θ'],                             # Alveolar fricative, voicing or assimilation to dental\n","    'z': ['s', 'z', 'ð'],                             # Voiced fricative, voiceless equivalent or dental substitution\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],                            # Postalveolar fricative, can become affricate or voiced version\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],                            # Voiced version, matches affricate and fricative pairs\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],                        # Nasals flexible depending on position or assimilation\n","    'ŋ': ['n', 'ŋ', 'ɲ'],                             # Velar nasal, interchangeable in word-final or medial\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],                        # Lateral approximants, dark \"l\" (syllable-final)\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],               # Rhotics, dropped or replaced with schwa in some dialects\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],                       # Voiceless bilabial stop, can become glottal or aspirated\n","    'b': ['p', 'b'],                                  # Voiced bilabial, softens or voices to pair\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],                       # Velar stop, can become glottal or aspirated\n","    'g': ['k', 'g'],                                  # Voiced velar, interchangeable with unvoiced equivalent\n","    'h': ['', 'h', 'ʔ'],                              # H-dropping and glottal substitution\n","    'f': ['f', 'v', 'θ'],                             # Fricative voicing lenition or substitution with dental\n","    'θ': ['θ', 'f', 't'],                             # Th-fronting or substitution with \"t\" in certain accents\n","    'ð': ['ð', 'v', 'd'],                             # Voiced equivalent, voicing changes or replaced by stop\n","    'j': ['', 'j', 'tʃ', 'dʒ'],                       # Yod-coalescence, yod-dropping, or affricate formation\n","    'w': ['w', 'ʍ'],                                  # Labialized velar or voiceless version (wh-sound)\n","    'v': ['v', 'f', 'w'],                             # Labio-dental voiced to voiceless substitution\n","    'm': ['m', 'n'],                                  # Labial to alveolar nasal in connected speech\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],                                   # Can be a gap or silent in blending two words\n","    'ɾ': ['t', 'd', 'ɾ'],                             # Flap, typical in American English (e.g., butter)\n","    'ʊ': ['ʊ', 'ɫ'],                                  # Dark l as final sound alternative (syllabic l)\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],                                # Aspirated or unaspirated based on context\n","    'tʰ': ['t', 'tʰ'],                                # Alveolar, aspirated/unaspirated\n","    'kʰ': ['k', 'kʰ'],                                # Velar, aspirated/unaspirated\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],                    # Diphthong reduced to monophthong in some accents\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],                           # Diphthong reduction or smoothening\n","    'aʊ': ['aʊ', 'æ', 'o'],                           # Dipthong reduced based on accent or emphasis\n","    'eɪ': ['eɪ', 'e', 'ɛ'],                           # Reduction or merging with other vowels\n","    'eə': ['eə', 'ɜ', 'ə'],                           # Reduction to schwa or smoothened articulation\n","    'ʌɪ': ['aɪ', 'aː'],                               # Similar in diphthongs, common in dialect shift\n","\n","    # Reduction to Schwa and Stress Variability\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],                        # Schwa replaced with context-specific vowels\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],                            # Stressed/unstressed variations based on regional stress\n","\n","    # Lateralization and Coarticulation\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],                          # Affricate reduced to fricative or voiced counterpart\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],                          # Affricate voiced/voiceless interplay\n","    'tw': ['t', 'tw'],                                # Labialization and added rounding (e.g., \"twenty\")\n","}\n","\n","### Additional Linguistic Features & Considerations\n","\n","rules = {\n","    \"aspiration_lenition_fortition\": lambda p1, surrounding: p1 in ['p', 't', 'k'] and surrounding in ['pʰ', 'tʰ', 'kʰ'],\n","    \"glottal_stops_for_gaps\": lambda p1, p2: (p1 == '' or p2 == 'ʔ'),\n","    \"intrusive_linking_r\": lambda p1, p2: p1 == 'r' or p2 == 'r',\n","    \"th_fronting\": lambda p1, p2: p1 == 'θ' and p2 in ['f', 't'],\n","    \"dark_l_vocalized_l\": lambda p1, position: p1 == 'l' and position == 'syllable-final',\n","    \"vowel_reduction_schwa\": lambda p1, position: p1 in ['a', 'e', 'o', 'u'] and position == 'unstressed',\n","    \"nasal_place_assimilation\": lambda p1, next_p: p1 == 'n' and next_p in ['p', 'b', 'm'],\n","    \"flapping\": lambda p1, p2: p1 in ['t', 'd'] and p2 == 'ɾ',\n","    \"coarticulation_effects\": lambda p1, context: p1 in ['h'] and context == 'casual',\n","    \"h_dropping\": lambda p1, position: p1 == 'h' and position == 'unstressed',\n","    \"nasal_g_dropping\": lambda p1, position: p1 == 'ŋ' and position == 'word-final',\n","   \"affricate_fricative_coalescence\": lambda p1, p2: (p1 == 'tʃ' and p2 == 'ʃ') or (p1 == 'dʒ' and p2 == 'ʒ'),\n","    \"prosodic_markers_stress_patterns\": lambda stress: stress in ['primary', 'secondary'],  # Recognizing stress levels\n","    \"glottal_reinforcement\": lambda p1, stress: p1 == 'ʔ' and stress == 'stressed',\n","    \"meter_tempo_accentuation\": lambda context: context in ['poetry', 'lyrical'],  # Applying rhythmic modifications\n","    \"linking_r\": lambda p1, p2: p1 == 'r' or p2 == 'r',  # Linking R in non-rhotic accents\n","    \"vowel_harmony\": lambda p1, p2: p1 in ['i', 'e'] and p2 in ['i', 'e'],  # Vowel harmonization\n","     \"devoicing_final_consonants\": lambda p1, position: p1 in ['b', 'd', 'g'] and position == 'final',  # Devoicing at end of word\n","}"],"metadata":{"id":"rMttC-tHCUQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reference file for phoneme mapping\n","\n","# Consolidated phoneme mapping with advanced linguistic rules\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],                          # High front vowels, long/short interchange\n","    'ɪ': ['i', 'ɪ'],                                 # Close front vowel, similar in many accents\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],                    # Diphthong mapping to monophthongs or other diphthongs\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],                      # Mid front, flexible in accents (e.g., Northern English \"e\" vs \"æ\")\n","    'æ': ['a', 'æ', 'ɑ'],                             # Low front, often replaced by \"a\" or \"ɑ\" in regional accents\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],                        # Open back, overlaps with \"a\" and rounded in some dialects\n","    'ɔ': ['o', 'ɔ', 'ɒ'],                             # Mid back, can interchange with \"o\" or low back rounded vowel\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],                        # Schwa, interchangeable in unstressed positions\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],                             # Open-mid back, often overlaps with schwa\n","    'u': ['u', 'ʊ'],                                  # High back, interchanges with rounded versions\n","    'ʊ': ['u', 'ʊ', 'oʊ'],                            # Close back rounded, also maps with certain diphthongs\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],                             # Mid central vowel, similar to schwa or rhotic version\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],                        # Back rounded, can vary regionally to open front\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],                       # Alveolar stop, changes to glottal stop, flap, or labialized variant\n","    'd': ['t', 'ɾ', 'd'],                             # Alveolar stop, interchange with voiced or voiceless versions\n","    's': ['s', 'z', 'θ'],                             # Alveolar fricative, voicing or assimilation to dental\n","    'z': ['s', 'z', 'ð'],                             # Voiced fricative, voiceless equivalent or dental substitution\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],                            # Postalveolar fricative, can become affricate or voiced version\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],                            # Voiced version, matches affricate and fricative pairs\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],                        # Nasals flexible depending on position or assimilation\n","    'ŋ': ['n', 'ŋ', 'ɲ'],                             # Velar nasal, interchangeable in word-final or medial\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],                        # Lateral approximants, dark \"l\" (syllable-final)\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],               # Rhotics, dropped or replaced with schwa in some dialects\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],                       # Voiceless bilabial stop, can become glottal or aspirated\n","    'b': ['p', 'b'],                                  # Voiced bilabial, softens or voices to pair\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],                       # Velar stop, can become glottal or aspirated\n","    'g': ['k', 'g'],                                  # Voiced velar, interchangeable with unvoiced equivalent\n","    'h': ['', 'h', 'ʔ'],                              # H-dropping and glottal substitution\n","    'f': ['f', 'v', 'θ'],                             # Fricative voicing lenition or substitution with dental\n","    'θ': ['θ', 'f', 't'],                             # Th-fronting or substitution with \"t\" in certain accents\n","    'ð': ['ð', 'v', 'd'],                             # Voiced equivalent, voicing changes or replaced by stop\n","    'j': ['', 'j', 'tʃ', 'dʒ'],                       # Yod-coalescence, yod-dropping, or affricate formation\n","    'w': ['w', 'ʍ'],                                  # Labialized velar or voiceless version (wh-sound)\n","    'v': ['v', 'f', 'w'],                             # Labio-dental voiced to voiceless substitution\n","    'm': ['m', 'n'],                                  # Labial to alveolar nasal in connected speech\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],                                   # Can be a gap or silent in blending two words\n","    'ɾ': ['t', 'd', 'ɾ'],                             # Flap, typical in American English (e.g., butter)\n","    'ʊ': ['ʊ', 'ɫ'],                                  # Dark l as final sound alternative (syllabic l)\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],                                # Aspirated or unaspirated based on context\n","    'tʰ': ['t', 'tʰ'],                                # Alveolar, aspirated/unaspirated\n","    'kʰ': ['k', 'kʰ'],                                # Velar, aspirated/unaspirated\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],                    # Diphthong reduced to monophthong in some accents\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],                           # Diphthong reduction or smoothening\n","    'aʊ': ['aʊ', 'æ', 'o'],                           # Dipthong reduced based on accent or emphasis\n","    'eɪ': ['eɪ', 'e', 'ɛ'],                           # Reduction or merging with other vowels\n","    'eə': ['eə', 'ɜ', 'ə'],                           # Reduction to schwa or smoothened articulation\n","    'ʌɪ': ['aɪ', 'aː'],                               # Similar in diphthongs, common in dialect shift\n","\n","    # Reduction to Schwa and Stress Variability\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],                        # Schwa replaced with context-specific vowels\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],                            # Stressed/unstressed variations based on regional stress\n","\n","    # Lateralization and Coarticulation\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],                          # Affricate reduced to fricative or voiced counterpart\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],                          # Affricate voiced/voiceless interplay\n","    'tw': ['t', 'tw'],                                # Labialization and added rounding (e.g., \"twenty\")\n","}\n","\n","### Additional Linguistic Features & Considerations\n","\n","rules = {\n","    \"aspiration_lenition_fortition\": lambda p1, surrounding: p1 in ['p', 't', 'k'] and surrounding in ['pʰ', 'tʰ', 'kʰ'],\n","    \"glottal_stops_for_gaps\": lambda p1, p2: (p1 == '' or p2 == 'ʔ'),\n","    \"intrusive_linking_r\": lambda p1, p2: p1 == 'r' or p2 == 'r',\n","    \"th_fronting\": lambda p1, p2: p1 == 'θ' and p2 in ['f', 't'],\n","    \"dark_l_vocalized_l\": lambda p1, position: p1 == 'l' and position == 'syllable-final',\n","    \"vowel_reduction_schwa\": lambda p1, position: p1 in ['a', 'e', 'o', 'u'] and position == 'unstressed',\n","    \"nasal_place_assimilation\": lambda p1, next_p: p1 == 'n' and next_p in ['p', 'b', 'm'],\n","    \"flapping\": lambda p1, p2: p1 in ['t', 'd'] and p2 == 'ɾ',\n","    \"coarticulation_effects\": lambda p1, context: p1 in ['h'] and context == 'casual',\n","    \"h_dropping\": lambda p1, position: p1 == 'h' and position == 'unstressed',\n","    \"nasal_g_dropping\": lambda p1, position: p1 == 'ŋ' and position == 'word-final',\n","   \"affricate_fricative_coalescence\": lambda p1, p2: (p1 == 'tʃ' and p2 == 'ʃ') or (p1 == 'dʒ' and p2 == 'ʒ'),\n","    \"prosodic_markers_stress_patterns\": lambda stress: stress in ['primary', 'secondary'],  # Recognizing stress levels\n","    \"glottal_reinforcement\": lambda p1, stress: p1 == 'ʔ' and stress == 'stressed',\n","    \"meter_tempo_accentuation\": lambda context: context in ['poetry', 'lyrical'],  # Applying rhythmic modifications\n","    \"linking_r\": lambda p1, p2: p1 == 'r' or p2 == 'r',  # Linking R in non-rhotic accents\n","    \"vowel_harmony\": lambda p1, p2: p1 in ['i', 'e'] and p2 in ['i', 'e'],  # Vowel harmonization\n","     \"devoicing_final_consonants\": lambda p1, position: p1 in ['b', 'd', 'g'] and position == 'final',  # Devoicing at end of word\n","}"],"metadata":{"id":"kV9VdfkdCQEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content')\n"],"metadata":{"id":"wBxKmZui_49g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TLFA5tDHDA71"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"JiMOA48J-1Uk"}},{"cell_type":"code","source":["from /content/phoneme_mapping_reference_.py import phoneme_mapping, rules"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"v2J-QAWbBluY","executionInfo":{"status":"error","timestamp":1729009994531,"user_tz":-60,"elapsed":588,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"a167f6c2-629a-4cf2-d893-76c9258682ca"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-37-f531ab2d067a>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-f531ab2d067a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from /content/phoneme_mapping_reference_.py import phoneme_mapping, rules\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","import psutil\n","import time\n","\n","# Comprehensive Phoneme Mapping Dictionary\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],\n","    'æ': ['a', 'æ', 'ɑ'],\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],\n","    'ɔ': ['o', 'ɔ', 'ɒ'],\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],\n","    'u': ['u', 'ʊ'],\n","    'ʊ': ['u', 'ʊ', 'oʊ'],\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],\n","    'd': ['t', 'ɾ', 'd'],\n","    's': ['s', 'z', 'θ'],\n","    'z': ['s', 'z', 'ð'],\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],\n","    'ŋ': ['n', 'ŋ', 'ɲ'],\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],\n","    'g': ['k', 'g'],\n","    'h': ['', 'h', 'ʔ'],\n","    'f': ['f', 'v', 'θ'],\n","    'θ': ['θ', 'f', 't'],\n","    'ð': ['ð', 'v', 'd'],\n","    'j': ['', 'j', 'tʃ', 'dʒ'],\n","    'w': ['w', 'ʍ'],\n","    'v': ['v', 'f', 'w'],\n","    'm': ['m', 'n'],\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],\n","    'ɾ': ['t', 'd', 'ɾ'],\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],\n","    'tʰ': ['t', 'tʰ'],\n","    'kʰ': ['k', 'kʰ'],\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],\n","    'aʊ': ['aʊ', 'æ', 'o'],\n","    'eɪ': ['eɪ', 'e', 'ɛ'],\n","    'eə': ['eə', 'ɜ', 'ə'],\n","    'ʌɪ': ['aɪ', 'aː'],\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],\n","    'tw': ['t', 'tw']\n","}\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.txt')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to apply rules for matching phonemes\n","def match_using_rules(phoneme1, phoneme2):\n","    # Check if the phonemes match based on predefined phoneme mapping rules\n","    options = phoneme_mapping.get(phoneme1, [phoneme1])\n","    return phoneme2 in options\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find advanced matches including partial, multi-word, and combinations\n","def find_advanced_matches(word_pairs, similarity_threshold=0.85):\n","    matches = []\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","\n","  # Example usage to find matches\n","vocabulary = list(phonetic_dict.keys())\n","word_pairs = [(word1, word2) for word1 in vocabulary for word2 in vocabulary if word1 != word2]\n","\n","# Now you can use word_pairs in the loop\n","batch_size = 2\n","\n","for idx in tqdm(range(0, len(word_pairs), batch_size), desc=\"Finding matches in batches\"):\n","    # Monitor memory usage and adjust if necessary\n","    memory_usage = psutil.virtual_memory().percent\n","    if memory_usage > 50:  # If memory usage exceeds 50%, pause to prevent overload\n","        print(f\"Memory usage too high ({memory_usage}%). Pausing for 10 seconds...\")\n","        time.sleep(10)\n","        memory_usage = psutil.virtual_memory().percent\n","        if memory_usage > 50:\n","            print(\"Memory usage still high. Reducing batch size to avoid crash.\")\n","            batch_size = max(batch_size // 2, 100)\n","\n","    # Process the current batch of word pairs\n","    current_batch = word_pairs[idx:idx + batch_size]\n","\n","    for word1, word2 in current_batch:\n","        phoneme_seq1 = get_phonemes(word1)\n","        phoneme_seq2 = get_phonemes(word2)\n","        similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","        match_type = \"\"\n","\n","        # Continue with your matching logic...\n","\n","\n","\n","    # Existing logic for finding matches (loop through `current_batch` and process word pairs)\n","    for word1, word2 in current_batch:\n","        # Your existing code here, for matching word1 and word2\n","\n","            phoneme_seq1 = get_phonemes(word1)\n","            phoneme_seq2 = get_phonemes(word2)\n","            similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","            match_type = \"\"\n","\n","            # Full word match\n","            if similarity >= similarity_threshold:\n","                match_type = \"Full word match\"\n","                matches.append((word1, word2, similarity, match_type))\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Partial word matches\n","            if len(phoneme_seq1) > 0 and len(phoneme_seq2) > 0:\n","                len1, len2 = len(phoneme_seq1), len(phoneme_seq2)\n","                shorter_len = min(len1, len2)\n","\n","                if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                    match_type = \"Partial word match\"\n","                    matches.append((word1, word2, similarity, match_type))\n","                    csv_writer.writerow([word1, word2, similarity, match_type])\n","                    continue\n","\n","                # Allow combining partial word phonemes to reach at least 85% of the other word's length\n","                if shorter_len / max(len1, len2) >= 0.85:\n","                    match_type = \"Combined partial word match\"\n","                    matches.append((word1, word2, similarity, match_type))\n","                    csv_writer.writerow([word1, word2, similarity, match_type])\n","                    continue\n","\n","            # Multi-word matches by merging phonemes\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","            for other_word in phonetic_dict.keys():\n","                other_phonemes = get_phonemes(other_word)\n","                merged_similarity = phoneme_similarity(merged_phonemes, other_phonemes)\n","\n","                if merged_similarity >= similarity_threshold:\n","                    match_type = \"Multi-word match\"\n","                    matches.append((f\"{word1} {word2}\", other_word, merged_similarity, match_type))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, merged_similarity, match_type])\n","                    break\n","\n","            # Multi-word to multi-word match (considering permutations)\n","            multi_word_combinations = itertools.permutations(word_pairs, 2)\n","            for (w1, w2), (w3, w4) in multi_word_combinations:\n","                seq1 = merge_phonemes_with_gaps(w1, w2, ignore_gap=True)\n","                seq2 = merge_phonemes_with_gaps(w3, w4, ignore_gap=True)\n","\n","                combined_similarity = phoneme_similarity(seq1, seq2)\n","                if combined_similarity >= similarity_threshold:\n","                    match_type = \"Multi-word to multi-word match\"\n","                    matches.append((f\"{w1} {w2}\", f\"{w3} {w4}\", combined_similarity, match_type))\n","                    csv_writer.writerow([f\"{w1} {w2}\", f\"{w3} {w4}\", combined_similarity, match_type])\n","                    continue\n","\n","            # Apply linguistic tricks or salvage rules if phoneme mismatch can be ignored\n","            phonemes1_list = list(phoneme_seq1)\n","            phonemes2_list = list(phoneme_seq2)\n","\n","            for p1, p2 in zip(phonemes1_list, phonemes2_list):\n","                if not match_using_rules(p1, p2):\n","                    # Check if there's a salvageable rule from the mapping that allows ignoring the mismatch\n","                    if phoneme_mapping.get(p1) and p2 in phoneme_mapping[p1]:\n","                        match_type = \"Salvaged by phoneme rule\"\n","                        matches.append((word1, word2, similarity, match_type))\n","                        csv_writer.writerow([word1, word2, similarity, match_type])\n"],"metadata":{"id":"kZs-HdsXDCc7","colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"status":"error","timestamp":1737896389778,"user_tz":0,"elapsed":199,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"fc0444e7-f6fe-42af-c0be-eb2d5400fee3"},"execution_count":4,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 1 fields in line 57815, saw 2\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-030b5cdc5d1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Load the phonetic dictionary from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/en_UK.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mphonetic_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aah'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/ˈɑː/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming column A contains words and B contains phonetic transcriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 57815, saw 2\n"]}]},{"source":["import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","import psutil\n","import time\n","\n","# ... (Existing code) ...\n","\n","# Create a list of word pairs (Assuming you want all possible pairs from 'phonetic_dict')\n","# If you have a different source for word pairs, please replace this line.\n","word_pairs = list(itertools.combinations(phonetic_dict.keys(), 2))\n","\n","# ... (Rest of the code) ..."],"cell_type":"code","metadata":{"id":"-dq9kBYdGf7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install psutil\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_6XD9QkEm2J","executionInfo":{"status":"ok","timestamp":1729010763903,"user_tz":-60,"elapsed":4073,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"a8f1afde-f957-46aa-ffd9-a394aad2e487"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","import psutil\n","import time\n","\n","# Comprehensive Phoneme Mapping Dictionary\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],\n","    'æ': ['a', 'æ', 'ɑ'],\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],\n","    'ɔ': ['o', 'ɔ', 'ɒ'],\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],\n","    'u': ['u', 'ʊ'],\n","    'ʊ': ['u', 'ʊ', 'oʊ'],\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],\n","    'd': ['t', 'ɾ', 'd'],\n","    's': ['s', 'z', 'θ'],\n","    'z': ['s', 'z', 'ð'],\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],\n","    'ŋ': ['n', 'ŋ', 'ɲ'],\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],\n","    'g': ['k', 'g'],\n","    'h': ['', 'h', 'ʔ'],\n","    'f': ['f', 'v', 'θ'],\n","    'θ': ['θ', 'f', 't'],\n","    'ð': ['ð', 'v', 'd'],\n","    'j': ['', 'j', 'tʃ', 'dʒ'],\n","    'w': ['w', 'ʍ'],\n","    'v': ['v', 'f', 'w'],\n","    'm': ['m', 'n'],\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],\n","    'ɾ': ['t', 'd', 'ɾ'],\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],\n","    'tʰ': ['t', 'tʰ'],\n","    'kʰ': ['k', 'kʰ'],\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],\n","    'aʊ': ['aʊ', 'æ', 'o'],\n","    'eɪ': ['eɪ', 'e', 'ɛ'],\n","    'eə': ['eə', 'ɜ', 'ə'],\n","    'ʌɪ': ['aɪ', 'aː'],\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],\n","    'tw': ['t', 'tw']\n","}\n","\n","    # ... (all phoneme mapping here, same as your original code)\n","\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.txt')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    if not isinstance(word, str):\n","        return ''\n","    return phonetic_dict.get(word.lower(), '')\n","\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find advanced matches including partial, multi-word, and combinations\n","def find_advanced_matches(vocabulary, similarity_threshold=0.85):\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","\n","        # Using a generator to create word pairs on demand\n","        word_pairs = itertools.combinations(vocabulary, 2)\n","\n","        batch_size = 1000  # Start with a manageable batch size\n","\n","        # Iterate over word pairs in small batches\n","        current_batch = []\n","        for word1, word2 in tqdm(word_pairs, desc=\"Finding matches\"):\n","            # Skip if no shared phonemes (basic optimization)\n","            phonemes1 = set(get_phonemes(word1))\n","            phonemes2 = set(get_phonemes(word2))\n","            if not phonemes1.intersection(phonemes2):\n","                continue\n","\n","            # Add pair to the current batch\n","            current_batch.append((word1, word2))\n","\n","            # Process the batch if it reaches the batch size\n","            if len(current_batch) >= batch_size:\n","                process_batch(current_batch, csv_writer, similarity_threshold)\n","                current_batch = []  # Reset the batch\n","\n","                # Monitor memory usage and adjust batch size if needed\n","                memory_usage = psutil.virtual_memory().percent\n","                if memory_usage > 50:  # If memory usage exceeds 50%, adjust\n","                    print(f\"Memory usage too high ({memory_usage}%). Adjusting batch size.\")\n","                    batch_size = max(batch_size // 2, 50)\n","\n","        # Process any remaining pairs in the last batch\n","        if current_batch:\n","            process_batch(current_batch, csv_writer, similarity_threshold)\n","\n","# Function to process a batch of word pairs and write matches to CSV\n","def process_batch(batch, csv_writer, similarity_threshold):\n","    for word1, word2 in batch:\n","        phoneme_seq1 = get_phonemes(word1)\n","        phoneme_seq2 = get_phonemes(word2)\n","        similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","        match_type = \"\"\n","\n","        # Full word match\n","        if similarity >= similarity_threshold:\n","            match_type = \"Full word match\"\n","            csv_writer.writerow([word1, word2, similarity, match_type])\n","            continue\n","\n","        # Partial word matches\n","        if len(phoneme_seq1) > 0 and len(phoneme_seq2) > 0:\n","            len1, len2 = len(phoneme_seq1), len(phoneme_seq2)\n","            shorter_len = min(len1, len2)\n","\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Allow combining partial word phonemes to reach at least 85% of the other word's length\n","            if shorter_len / max(len1, len2) >= 0.85:\n","                match_type = \"Combined partial word match\"\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","        # Multi-word matches by merging phonemes\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","        for other_word in phonetic_dict.keys():\n","            other_phonemes = get_phonemes(other_word)\n","            merged_similarity = phoneme_similarity(merged_phonemes, other_phonemes)\n","\n","            if merged_similarity >= similarity_threshold:\n","                match_type = \"Multi-word match\"\n","                csv_writer.writerow([f\"{word1} {word2}\", other_word, merged_similarity, match_type])\n","                break\n","\n","# Run the matching process\n","vocabulary = list(phonetic_dict.keys())\n","find_advanced_matches(vocabulary, similarity_threshold=0.85)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"HWSNO6LLJuIN","executionInfo":{"status":"error","timestamp":1737898334721,"user_tz":0,"elapsed":487,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"9150cee4-014e-4dce-a448-ec395f79975b"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 1 fields in line 57815, saw 2\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5de007d72682>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Load the phonetic dictionary from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/en_UK.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mphonetic_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aah'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/ˈɑː/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming column A contains words and B contains phonetic transcriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 57815, saw 2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wyhz7ylf4n8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install TTS\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"5aPlJwZlIVRP","executionInfo":{"status":"ok","timestamp":1737903703322,"user_tz":0,"elapsed":41240,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"6c9c4c79-68a1-456a-e2e9-e60fa60496f2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting TTS\n","  Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl.metadata (21 kB)\n","Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.0.11)\n","Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.13.1)\n","Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (2.5.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from TTS) (2.5.1+cu121)\n","Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.13.0)\n","Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.10.2.post1)\n","Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.6.1)\n","Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (7.5.0)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.67.1)\n","Collecting anyascii>=0.3.0 (from TTS)\n","  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (6.0.2)\n","Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (2024.10.0)\n","Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.11.11)\n","Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (24.2)\n","Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.1.0)\n","Collecting pysbd>=0.3.4 (from TTS)\n","  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n","Collecting umap-learn>=0.5.1 (from TTS)\n","  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n","Collecting pandas<2.0,>=1.4 (from TTS)\n","  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.10.0)\n","Collecting trainer>=0.0.32 (from TTS)\n","  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n","Collecting coqpit>=0.0.16 (from TTS)\n","  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from TTS) (0.42.1)\n","Collecting pypinyin (from TTS)\n","  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n","Collecting hangul-romanize (from TTS)\n","  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading gruut-2.2.3.tar.gz (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jamo (from TTS)\n","  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from TTS) (3.9.1)\n","Collecting g2pkk>=0.1.1 (from TTS)\n","  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Collecting bangla (from TTS)\n","  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n","Collecting bnnumerizer (from TTS)\n","  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bnunicodenormalizer (from TTS)\n","  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.8.0)\n","Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.47.1)\n","Collecting encodec>=0.1.1 (from TTS)\n","  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting unidecode>=1.3.2 (from TTS)\n","  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n","Collecting num2words (from TTS)\n","  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n","Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.26.4)\n","Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.60.0)\n","Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.16.0)\n","Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n","Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n","Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n","  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.18.3)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.5)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n","Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (10.5.0)\n","Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (4.4.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.12.2)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (4.55.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n","Collecting docopt>=0.6.2 (from num2words->TTS)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.0->TTS) (0.43.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0,>=1.4->TTS) (2024.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.15.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.10.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.1.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.5.0)\n","Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n","  Downloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n","  Downloading SudachiDict_core-20241021-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.17.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->TTS) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (2.17.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.5.2)\n","Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n","  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (3.0.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.17.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.3.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->TTS) (4.3.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.1.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.69.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.5)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n","Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl (937 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n","Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n","Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m138.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n","Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n","Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n","Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n","Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n","  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=7c8551cc5a984ab3a8affdccb0d53914e7a4effc2ae79ede07ba21216635e02d\n","  Stored in directory: /root/.cache/pip/wheels/1f/a0/bc/4dacab52579ab464cffafbe7a8e3792dd36ad9ac288b264843\n","  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=32426a349ca26ee2a8dab4850df8a2265a07f34732f93ceb71e4f71cdd1707b2\n","  Stored in directory: /root/.cache/pip/wheels/b4/a4/88/480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\n","  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=09f2d782803107c2d15a02f14c40119e2901b86b227cc5e87ba7449d03712a35\n","  Stored in directory: /root/.cache/pip/wheels/9e/b9/e3/4145416693824818c0b931988a692676ecd4bbf2ea41d1eedd\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=db50d41acd828081d8b7fa28dd096fcab45016f2b9a8c262798561fee81f2c32\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=4cbca8a7fe964f9f5e8bc704f3cf8671e2ceac5c374f0fbed106b4d96658e2fe\n","  Stored in directory: /root/.cache/pip/wheels/c7/10/89/a5908dd7a9a032229684b7679396785e19f816667f788087fb\n","  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498313 sha256=2c3acd880949382bf0e7ff11da89332670a7e598aa6e4592974c2f14afdfd99c\n","  Stored in directory: /root/.cache/pip/wheels/87/fa/df/5fdf5d3cc26ba859b8698a1f28581d1a6aa081edc6df9847ab\n","  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=574b32795de68ccab44670cdc6668cb7c3913f5f524994c4cb2e68b334ddba81\n","  Stored in directory: /root/.cache/pip/wheels/06/30/52/dc5cd222b4bbde285838fed1f96636e96f85cd75493e79a978\n","  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=93720b78f2c50a9c5b6fc3b3b45db3326c73470af43bbd482925664d30e471a6\n","  Stored in directory: /root/.cache/pip/wheels/c8/eb/59/30b5d15e56347e595f613036cbea0f807ad9621c75cd75d912\n","  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=bd0180a33522687c722e8a5b0a7e52678f8e120289e374d85bc81c95389b848c\n","  Stored in directory: /root/.cache/pip/wheels/e0/e7/a0/7c416a3eeaa94ca71bf7bcbc6289cced2263d8ba35e82444bb\n","Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n","Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-crfsuite, pysbd, pypinyin, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, pandas, g2pkk, dateparser, pynndescent, gruut, umap-learn, trainer, encodec, TTS\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.4.2\n","    Uninstalling networkx-3.4.2:\n","      Successfully uninstalled networkx-3.4.2\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","scikit-image 0.25.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n","nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n","plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","xarray 2025.1.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n","cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 networkx-2.8.8 num2words-0.5.14 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 sudachidict-core-20241021 sudachipy-0.6.10 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pandas"]},"id":"5b0a15c722b74b5384d03757f2e937ab"}},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","import psutil\n","import time\n","import multiprocessing\n","import signal\n","\n","# Comprehensive Phoneme Mapping Dictionary\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],\n","    'æ': ['a', 'æ', 'ɑ'],\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],\n","    'ɔ': ['o', 'ɔ', 'ɒ'],\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],\n","    'u': ['u', 'ʊ'],\n","    'ʊ': ['u', 'ʊ', 'oʊ'],\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],\n","    'd': ['t', 'ɾ', 'd'],\n","    's': ['s', 'z', 'θ'],\n","    'z': ['s', 'z', 'ð'],\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],\n","    'ŋ': ['n', 'ŋ', 'ɲ'],\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],\n","    'g': ['k', 'g'],\n","    'h': ['', 'h', 'ʔ'],\n","    'f': ['f', 'v', 'θ'],\n","    'θ': ['θ', 'f', 't'],\n","    'ð': ['ð', 'v', 'd'],\n","    'j': ['', 'j', 'tʃ', 'dʒ'],\n","    'w': ['w', 'ʍ'],\n","    'v': ['v', 'f', 'w'],\n","    'm': ['m', 'n'],\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],\n","    'ɾ': ['t', 'd', 'ɾ'],\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],\n","    'tʰ': ['t', 'tʰ'],\n","    'kʰ': ['k', 'kʰ'],\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],\n","    'aʊ': ['aʊ', 'æ', 'o'],\n","    'eɪ': ['eɪ', 'e', 'ɛ'],\n","    'eə': ['eə', 'ɜ', 'ə'],\n","    'ʌɪ': ['aɪ', 'aː'],\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],\n","    'tw': ['t', 'tw']\n","}\n","\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    if not isinstance(word, str):\n","        return ''\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    if phoneme_seq1 == phoneme_seq2:  # Early exit if identical\n","        return 1.0\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to find advanced matches including partial, multi-word, and combinations\n","def find_advanced_matches(vocabulary, similarity_threshold=0.85):\n","    last_save_time = time.time()  # Initialize save timestamp\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","\n","        # Using a generator to create word pairs on demand\n","        word_pairs = itertools.combinations(vocabulary, 2)\n","\n","        batch_size = 1000  # Start with a manageable batch size\n","        current_batch = []\n","\n","        # Progress bar\n","        with tqdm(total=len(vocabulary) * (len(vocabulary) - 1) // 2, desc=\"Finding matches\") as pbar:\n","            for word1, word2 in word_pairs:\n","                # Skip if phonemes are missing (skip pairs that are not in the dictionary)\n","                phonemes1 = get_phonemes(word1)\n","                phonemes2 = get_phonemes(word2)\n","                if not phonemes1 or not phonemes2:\n","                    continue\n","\n","                # Add pair to the current batch\n","                current_batch.append((word1, word2))\n","\n","                # Process the batch if it reaches the batch size\n","                if len(current_batch) >= batch_size:\n","                    process_batch(current_batch, csv_writer, similarity_threshold)\n","                    current_batch = []  # Reset the batch\n","                    pbar.update(batch_size)  # Update progress bar\n","\n","                    # Monitor memory usage and adjust batch size if needed\n","                    memory_usage = psutil.virtual_memory().percent\n","                    if memory_usage > 50:  # If memory usage exceeds 50%, adjust\n","                        print(f\"Memory usage too high ({memory_usage}%). Adjusting batch size.\")\n","                        batch_size = max(batch_size // 2, 50)\n","\n","                    # Periodically save progress every 15 minutes\n","                    if time.time() - last_save_time > 900:\n","                        csvfile.flush()\n","                        last_save_time = time.time()\n","\n","            # Process any remaining pairs in the last batch\n","            if current_batch:\n","                process_batch(current_batch, csv_writer, similarity_threshold)\n","                pbar.update(len(current_batch))\n","\n","# Function to process a batch of word pairs and write matches to CSV\n","def process_batch(batch, csv_writer, similarity_threshold):\n","    for word1, word2 in batch:\n","        phoneme_seq1 = get_phonemes(word1)\n","        phoneme_seq2 = get_phonemes(word2)\n","        similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","        match_type = \"\"\n","\n","        # Full word match\n","        if similarity >= similarity_threshold:\n","            match_type = \"Full word match\"\n","            csv_writer.writerow([word1, word2, similarity, match_type])\n","            continue\n","\n","        # Partial word matches\n","        if len(phoneme_seq1) > 0 and len(phoneme_seq2) > 0:\n","            len1, len2 = len(phoneme_seq1), len(phoneme_seq2)\n","            shorter_len = min(len1, len2)\n","\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","            # Allow combining partial word phonemes to reach at least 85% of the other word's length\n","            if shorter_len / max(len1, len2) >= 0.85:\n","                match_type = \"Combined partial word match\"\n","                csv_writer.writerow([word1, word2, similarity, match_type])\n","                continue\n","\n","        # Multi-word matches by merging phonemes\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","        for other_word in phonetic_dict.keys():\n","            other_phonemes = get_phonemes(other_word)\n","            merged_similarity = phoneme_similarity(merged_phonemes, other_phonemes)\n","\n","            if merged_similarity >= similarity_threshold:\n","                match_type = \"Multi-word match\"\n","                csv_writer.writerow([f\"{word1} {word2}\", other_word, merged_similarity, match_type])\n","                break\n","\n","# Run the matching process\n","vocabulary = list(phonetic_dict.keys())\n","find_advanced_matches(vocabulary, similarity_threshold=0.85)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"ElzbPHMiMS7x","executionInfo":{"status":"error","timestamp":1729013040729,"user_tz":-60,"elapsed":186436,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"16f85e42-ac46-4dbb-f4d3-547c3f24785a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Finding matches:   0%|          | 0/2120014170 [03:05<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-8c9d04082c80>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m# Run the matching process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphonetic_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m \u001b[0mfind_advanced_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-8c9d04082c80>\u001b[0m in \u001b[0;36mfind_advanced_matches\u001b[0;34m(vocabulary, similarity_threshold)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;31m# Process the batch if it reaches the batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Reset the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-8c9d04082c80>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(batch, csv_writer, similarity_threshold)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mother_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphonetic_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mother_phonemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_phonemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mmerged_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoneme_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_phonemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_phonemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmerged_similarity\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-8c9d04082c80>\u001b[0m in \u001b[0;36mphoneme_similarity\u001b[0;34m(phoneme_seq1, phoneme_seq2)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mphoneme_seq1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mphoneme_seq2\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Early exit if identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_seq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_seq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Function to merge phonemes ignoring gaps for matching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/difflib.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \"\"\"\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_calculate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/difflib.py\u001b[0m in \u001b[0;36mget_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_longest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0;31m# a[alo:i] vs b[blo:j] unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# a[i:i+k] same as b[j:j+k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/difflib.py\u001b[0m in \u001b[0;36mfind_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mbestsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestsize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbesti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_cls, a, b, size)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","import psutil\n","import time\n","from concurrent.futures import ProcessPoolExecutor\n","\n","# Comprehensive Phoneme Mapping Dictionary\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],\n","    'æ': ['a', 'æ', 'ɑ'],\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],\n","    'ɔ': ['o', 'ɔ', 'ɒ'],\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],\n","    'u': ['u', 'ʊ'],\n","    'ʊ': ['u', 'ʊ', 'oʊ'],\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],\n","    'd': ['t', 'ɾ', 'd'],\n","    's': ['s', 'z', 'θ'],\n","    'z': ['s', 'z', 'ð'],\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],\n","    'ŋ': ['n', 'ŋ', 'ɲ'],\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],\n","    'g': ['k', 'g'],\n","    'h': ['', 'h', 'ʔ'],\n","    'f': ['f', 'v', 'θ'],\n","    'θ': ['θ', 'f', 't'],\n","    'ð': ['ð', 'v', 'd'],\n","    'j': ['', 'j', 'tʃ', 'dʒ'],\n","    'w': ['w', 'ʍ'],\n","    'v': ['v', 'f', 'w'],\n","    'm': ['m', 'n'],\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],\n","    'ɾ': ['t', 'd', 'ɾ'],\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],\n","    'tʰ': ['t', 'tʰ'],\n","    'kʰ': ['k', 'kʰ'],\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],\n","    'aʊ': ['aʊ', 'æ', 'o'],\n","    'eɪ': ['eɪ', 'e', 'ɛ'],\n","    'eə': ['eə', 'ɜ', 'ə'],\n","    'ʌɪ': ['aɪ', 'aː'],\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],\n","    'tw': ['t', 'tw']\n","}\n","\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Define a function to extract phonemes from a word using the phonetic dictionary\n","def get_phonemes(word):\n","    if not isinstance(word, str):\n","        return ''\n","    return phonetic_dict.get(word.lower(), '')\n","\n","# Function to calculate similarity between two phoneme sequences\n","def phoneme_similarity(phoneme_seq1, phoneme_seq2):\n","    return SequenceMatcher(None, phoneme_seq1, phoneme_seq2).ratio()\n","\n","# Function to filter pairs based on at least 50% shared phonemes ignoring gaps\n","def shared_phoneme_filter(phoneme_seq1, phoneme_seq2):\n","    set1 = set(phoneme_seq1.replace('ʔ', ''))  # Remove gaps and convert to set\n","    set2 = set(phoneme_seq2.replace('ʔ', ''))\n","    shared = len(set1.intersection(set2))\n","    return shared / min(len(set1), len(set2)) >= 0.5\n","\n","# Function to merge phonemes ignoring gaps for matching\n","def merge_phonemes_with_gaps(word1, word2, ignore_gap=True):\n","    phonemes1 = get_phonemes(word1)\n","    phonemes2 = get_phonemes(word2)\n","    if ignore_gap:\n","        merged_phonemes = phonemes1 + phonemes2  # Ignore gap\n","    else:\n","        merged_phonemes = phonemes1 + ' ʔ ' + phonemes2  # Gap as glottal stop\n","    return merged_phonemes\n","\n","# Function to run matches in parallel using multiprocessing\n","def process_batch_in_parallel(batch):\n","    results = []\n","    for word1, word2 in batch:\n","        phoneme_seq1 = get_phonemes(word1)\n","        phoneme_seq2 = get_phonemes(word2)\n","\n","        # Apply 50% shared phoneme filter\n","        if not shared_phoneme_filter(phoneme_seq1, phoneme_seq2):\n","            continue\n","\n","        similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","        match_type = \"\"\n","\n","        # Full word match\n","        if similarity >= 0.85:\n","            match_type = \"Full word match\"\n","            results.append((word1, word2, similarity, match_type))\n","\n","        # Partial word matches\n","        if len(phoneme_seq1) > 0 and len(phoneme_seq2) > 0:\n","            len1, len2 = len(phoneme_seq1), len(phoneme_seq2)\n","            shorter_len = min(len1, len2)\n","\n","            if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                match_type = \"Partial word match\"\n","                results.append((word1, word2, similarity, match_type))\n","                continue\n","\n","            # Allow combining partial word phonemes to reach at least 85% of the other word's length\n","            if shorter_len / max(len1, len2) >= 0.85:\n","                match_type = \"Combined partial word match\"\n","                results.append((word1, word2, similarity, match_type))\n","                continue\n","\n","        # Multi-word matches by merging phonemes\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","        for other_word in phonetic_dict.keys():\n","            other_phonemes = get_phonemes(other_word)\n","            merged_similarity = phoneme_similarity(merged_phonemes, other_phonemes)\n","\n","            if merged_similarity >= 0.85:\n","                match_type = \"Multi-word match\"\n","                results.append((f\"{word1} {word2}\", other_word, merged_similarity, match_type))\n","                break\n","\n","    return results\n","\n","# Function to find matches using parallel processing and periodically save results\n","def find_advanced_matches_parallel(vocabulary, similarity_threshold=0.85):\n","    last_save_time = time.time()\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","\n","        # Using a generator to create word pairs on demand\n","        word_pairs = list(itertools.combinations(vocabulary, 2))\n","\n","        # Define batch size\n","        batch_size = 5000  # Larger batch size for parallel processing\n","\n","        # Progress bar\n","        with tqdm(total=len(word_pairs), desc=\"Finding matches\") as pbar:\n","            with ProcessPoolExecutor() as executor:\n","                # Split word_pairs into chunks and process them in parallel\n","                for idx in range(0, len(word_pairs), batch_size):\n","                    batch = word_pairs[idx:idx + batch_size]\n","\n","                    # Submit batch to the executor for parallel processing\n","                    future = executor.submit(process_batch_in_parallel, batch)\n","                    results = future.result()\n","\n","                    # Write the results to the CSV\n","                    for result in results:\n","                        csv_writer.writerow(result)\n","\n","                    pbar.update(len(batch))\n","\n","                    # Monitor memory usage\n","                    memory_usage = psutil.virtual_memory().percent\n","                    print(f\"Current memory usage: {memory_usage}%\")\n","                    if memory_usage > 50:\n","                        print(\"High memory usage, adjusting batch size to be safe.\")\n","                        batch_size = max(batch_size // 2, 500)\n","\n","                    # Periodically save progress every 15 minutes\n","                    if time.time() - last_save_time > 900:\n","                        csvfile.flush()\n","                        last_save_time = time.time()\n","\n","# Run the matching process\n","vocabulary = list(phonetic_dict.keys())\n","find_advanced_matches_parallel(vocabulary, similarity_threshold=0.85)\n"],"metadata":{"id":"8SgEQN85NUXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from difflib import SequenceMatcher\n","from tqdm import tqdm\n","import csv\n","import itertools\n","import psutil\n","import time\n","from concurrent.futures import ProcessPoolExecutor\n","import shelve\n","\n","\n","# Enhanced Phoneme Clusters Incorporating All Linguistic Rules\n","phoneme_clusters = {\n","    # Vowel Clusters (including vowel length, diphthongs, and reduction)\n","    'high_front_vowels': ['i', 'ɪ', 'iː'],\n","    'low_front_vowels': ['æ', 'a', 'ɛ'],\n","    'back_vowels': ['ɑ', 'ɔ', 'ɒ'],\n","    'central_vowels': ['ə', 'ʌ', 'ɜ'],\n","    'high_back_vowels': ['u', 'ʊ'],\n","    'diphthongs': ['aɪ', 'ɔɪ', 'aʊ', 'eɪ', 'eə'],\n","\n","    # Consonant Clusters\n","    'voiced_stops': ['b', 'd', 'g'],\n","    'voiceless_stops': ['p', 't', 'k', 'ʔ'],\n","    'aspirated_stops': ['pʰ', 'tʰ', 'kʰ'],\n","    'alveolar_flap': ['ɾ'],\n","    'fricatives_voiced': ['v', 'z', 'ʒ', 'ð'],\n","    'fricatives_voiceless': ['f', 's', 'ʃ', 'θ'],\n","    'affricates': ['tʃ', 'dʒ'],\n","    'nasals': ['m', 'n', 'ŋ'],\n","    'laterals': ['l', 'ɫ'],\n","    'rhotics': ['r', 'ɹ', 'ɻ', 'ɾ', 'ə'],\n","    'glides': ['j', 'w'],\n","    'glottal': ['ʔ', 'h'],\n","    'gap_fillers': ['ʔ', ''],\n","    'intrusive_elements': ['tw', 'ʍ']\n","}\n","\n","# Create a cluster lookup to find which cluster a phoneme belongs to\n","cluster_lookup = {}\n","for cluster, phonemes in phoneme_clusters.items():\n","    for phoneme in phonemes:\n","        cluster_lookup[phoneme] = cluster\n","\n","# Function to check if two phonemes are in the same cluster (ignoring spaces)\n","def are_phonemes_in_same_cluster(phoneme1, phoneme2):\n","    if phoneme1 in ['', ' '] or phoneme2 in ['', ' ']:\n","        return True  # Ignore spaces\n","    cluster1 = cluster_lookup.get(phoneme1)\n","    cluster2 = cluster_lookup.get(phoneme2)\n","    return cluster1 == cluster2\n","\n","# Function to match phonemes at the cluster level\n","def match_phonemes_cluster_level(phoneme_seq1, phoneme_seq2):\n","    for p1, p2 in zip(phoneme_seq1, phoneme_seq2):\n","        if not are_phonemes_in_same_cluster(p1, p2):\n","            return False  # Return False if phonemes do not belong to the same cluster\n","    return True\n","\n","# Function to match phoneme sequences based on rules and clusters\n","def match_phoneme_sequences_with_rules(seq1, seq2):\n","    if not match_phonemes_cluster_level(seq1, seq2):\n","        return False  # Cluster-level match must pass first\n","\n","    for phoneme1, phoneme2 in zip(seq1, seq2):\n","        if phoneme1 == phoneme2:\n","            continue\n","        if phoneme1 in phoneme_mapping and phoneme2 in phoneme_mapping[phoneme1]:\n","            continue  # Match based on phoneme mapping\n","        if phoneme1 == 'ʔ' or phoneme2 == 'ʔ':  # Allow glottal substitution\n","            continue\n","        return False\n","    return True\n","\n","\n","\n"," # Comprehensive Phoneme Mapping Dictionary\n","phoneme_mapping = {\n","    # Vowel Phonemes\n","    'i': ['i', 'ɪ', 'iː'],\n","    'ɪ': ['i', 'ɪ'],\n","    'eɪ': ['e', 'eɪ', 'ɛ', 'aɪ'],\n","    'ɛ': ['ɛ', 'e', 'æ', 'aɪ'],\n","    'æ': ['a', 'æ', 'ɑ'],\n","    'ɑ': ['a', 'ɑ', 'æ', 'ɔ'],\n","    'ɔ': ['o', 'ɔ', 'ɒ'],\n","    'ə': ['ə', 'ʌ', 'ɜ', 'ɨ'],\n","    'ʌ': ['ə', 'ʌ', 'ɑ'],\n","    'u': ['u', 'ʊ'],\n","    'ʊ': ['u', 'ʊ', 'oʊ'],\n","    'ɜ': ['ɜ', 'ə', 'ɝ'],\n","    'ɒ': ['ɑ', 'ɔ', 'ɒ', 'æ'],\n","\n","    # Consonant Phonemes\n","    't': ['t', 'ʔ', 'ɾ', 'tw'],\n","    'd': ['t', 'ɾ', 'd'],\n","    's': ['s', 'z', 'θ'],\n","    'z': ['s', 'z', 'ð'],\n","    'ʃ': ['ʃ', 'ʒ', 'tʃ'],\n","    'ʒ': ['ʃ', 'ʒ', 'dʒ'],\n","    'n': ['n', 'ŋ', 'ɱ', 'ʔ'],\n","    'ŋ': ['n', 'ŋ', 'ɲ'],\n","    'l': ['l', 'ɫ', 'ʟ', 'ʊ'],\n","    'r': ['r', 'ɹ', 'ɻ', 'ə', 'ɾ', ''],\n","    'p': ['p', 'b', 'pʰ', 'ʔ'],\n","    'b': ['p', 'b'],\n","    'k': ['k', 'g', 'kʰ', 'ʔ'],\n","    'g': ['k', 'g'],\n","    'h': ['', 'h', 'ʔ'],\n","    'f': ['f', 'v', 'θ'],\n","    'θ': ['θ', 'f', 't'],\n","    'ð': ['ð', 'v', 'd'],\n","    'j': ['', 'j', 'tʃ', 'dʒ'],\n","    'w': ['w', 'ʍ'],\n","    'v': ['v', 'f', 'w'],\n","    'm': ['m', 'n'],\n","\n","    # Glottal Stops and Gap-Filling\n","    'ʔ': ['ʔ', ''],\n","    'ɾ': ['t', 'd', 'ɾ'],\n","\n","    # Aspiration and Lenition\n","    'pʰ': ['p', 'pʰ'],\n","    'tʰ': ['t', 'tʰ'],\n","    'kʰ': ['k', 'kʰ'],\n","\n","    # Reduction and Intrusive Elements\n","    'aɪ': ['aɪ', 'eɪ', 'ɪ', 'aː'],\n","    'ɔɪ': ['ɔɪ', 'ɔ', 'o'],\n","    'aʊ': ['aʊ', 'æ', 'o'],\n","    'eɪ': ['eɪ', 'e', 'ɛ'],\n","    'eə': ['eə', 'ɜ', 'ə'],\n","    'ʌɪ': ['aɪ', 'aː'],\n","    'ə': ['ə', 'ɪ', 'a', 'ɜ'],\n","    'ɜ': ['ɜ', 'ə', 'ɜr'],\n","    'tʃ': ['tʃ', 'dʒ', 'ʃ'],\n","    'dʒ': ['tʃ', 'dʒ', 'ʒ'],\n","    'tw': ['t', 'tw']\n","}\n","\n","# Load the phonetic dictionary from the CSV file\n","df = pd.read_csv('/content/en_UK.csv')\n","phonetic_dict = dict(zip(df['aah'], df['/ˈɑː/']))  # Assuming column A contains words and B contains phonetic transcriptions\n","\n","# Function to find advanced matches incorporating clustering and rule matching\n","def find_advanced_matches(word_pairs):\n","    matches = []\n","    with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['Word 1', 'Word 2', 'Match Type'])\n","\n","        for word1, word2 in tqdm(word_pairs, desc=\"Finding matches with clustering and rules\"):\n","            phoneme_seq1 = get_phonemes(word1)\n","            phoneme_seq2 = get_phonemes(word2)\n","\n","            # Full word match based on cluster and rule matching\n","            if match_phoneme_sequences_with_rules(phoneme_seq1, phoneme_seq2):\n","                match_type = \"Full word match using clustering and rules\"\n","                matches.append((word1, word2, match_type))\n","                csv_writer.writerow([word1, word2, match_type])\n","                continue\n","\n","            # Additional matching for partial, multi-word, and combination matches\n","            # Partial word matches\n","            if len(phoneme_seq1) > 0 and len(phoneme_seq2) > 0:\n","                len1, len2 = len(phoneme_seq1), len(phoneme_seq2)\n","                shorter_len = min(len1, len2)\n","\n","                if phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1:\n","                    match_type = \"Partial word match\"\n","                    matches.append((word1, word2, match_type))\n","                    csv_writer.writerow([word1, word2, match_type])\n","                    continue\n","\n","                # Allow combining partial word phonemes to reach at least 85% of the other word's length\n","                if shorter_len / max(len1, len2) >= 0.85:\n","                    match_type = \"Combined partial word match\"\n","                    matches.append((word1, word2, match_type))\n","                    csv_writer.writerow([word1, word2, match_type])\n","                    continue\n","\n","            # Multi-word matches by merging phonemes\n","            merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","            for other_word in phonetic_dict.keys():\n","                other_phonemes = get_phonemes(other_word)\n","\n","                if match_phoneme_sequences_with_rules(merged_phonemes, other_phonemes):\n","                    match_type = \"Multi-word match\"\n","                    matches.append((f\"{word1} {word2}\", other_word, match_type))\n","                    csv_writer.writerow([f\"{word1} {word2}\", other_word, match_type])\n","                    break\n","\n","    return matches\n","\n","# Function to process each batch of word pairs in parallel\n","def process_batch_in_parallel(batch, cache):\n","    results = []\n","    for word1, word2 in tqdm(batch, desc=\"Processing Batch\"):\n","        cache_key = f\"{word1}_{word2}\"\n","\n","        # Check if result is already cached to save computation\n","        if cache_key in cache:\n","            results.append(cache[cache_key])\n","            continue\n","\n","        phoneme_seq1 = get_phonemes(word1)\n","        phoneme_seq2 = get_phonemes(word2)\n","\n","        # Apply 50% shared phoneme filter\n","        if not shared_phoneme_filter(phoneme_seq1, phoneme_seq2):\n","            continue\n","\n","        # Apply fuzzy ratio to further reduce unnecessary computations\n","        if fuzz.ratio(phoneme_seq1, phoneme_seq2) < 50:\n","            continue\n","\n","        similarity = phoneme_similarity(phoneme_seq1, phoneme_seq2)\n","\n","        # Full word or partial matches\n","        if similarity >= 0.85 or (phoneme_seq1 in phoneme_seq2 or phoneme_seq2 in phoneme_seq1):\n","            match_type = \"Full or Partial match\"\n","            match_result = (word1, word2, similarity, match_type)\n","            results.append(match_result)\n","            cache[cache_key] = match_result\n","\n","        # Multi-word matches by merging phonemes\n","        merged_phonemes = merge_phonemes_with_gaps(word1, word2, ignore_gap=True)\n","        for other_word in phonetic_dict.keys():\n","            other_phonemes = get_phonemes(other_word)\n","            merged_similarity = phoneme_similarity(merged_phonemes, other_phonemes)\n","\n","            if merged_similarity >= 0.85:\n","                match_type = \"Multi-word match\"\n","                match_result = (f\"{word1} {word2}\", other_word, merged_similarity, match_type)\n","                results.append(match_result)\n","                cache[cache_key] = match_result\n","                break\n","\n","    return results\n","\n","# Function to find matches using parallel processing and caching mechanism\n","def find_advanced_matches_parallel(vocabulary, similarity_threshold=0.85, cache_filename='phoneme_cache'):\n","    last_save_time = time.time()\n","\n","    # Use a shelf to store intermediate results to disk (fuzzy caching)\n","    with shelve.open(cache_filename) as cache:\n","        with open('phoneme_matches_detailed.csv', 'w', newline='') as csvfile:\n","            csv_writer = csv.writer(csvfile)\n","            csv_writer.writerow(['Word 1', 'Word 2', 'Similarity', 'Type of Match'])\n","\n","            word_pairs = list(itertools.combinations(vocabulary, 2))\n","\n","            # Define batch size\n","            batch_size = 20\n","\n","            # Track total progress with progress bar\n","            with tqdm(total=len(word_pairs), desc=\"Finding matches\") as pbar:\n","                with ProcessPoolExecutor() as executor:\n","                    for idx in range(0, len(word_pairs), batch_size):\n","                        batch = word_pairs[idx:idx + batch_size]\n","\n","                        # Submit the batch to be processed in parallel\n","                        future = executor.submit(process_batch_in_parallel, batch, cache)\n","                        results = future.result()\n","\n","                        # Write the results to the CSV\n","                        for result in results:\n","                            word1, word2, similarity, match_type = result\n","                            csv_writer.writerow(result)\n","\n","                        pbar.update(len(batch))\n","\n","                        # Monitor memory usage\n","                        memory_usage = psutil.virtual_memory().percent\n","                        tqdm.write(f\"Current memory usage: {memory_usage}%\")\n","                        if memory_usage > 50:\n","                            tqdm.write(\"High memory usage detected. Reducing batch size.\")\n","                            batch_size = max(batch_size // 2, 500)\n","\n","                        # Periodically save results every 15 minutes\n","                        if time.time() - last_save_time > 900:\n","                            csvfile.flush()\n","                            cache.sync()\n","                            last_save_time = time.time()\n","                            tqdm.write(\"Progress saved to CSV and cache.\")\n","\n","# Run the matching process with vocabulary\n","vocabulary = list(phonetic_dict.keys())\n","find_advanced_matches_parallel(vocabulary, similarity_threshold=0.85)"],"metadata":{"id":"Oo3-kyu_Oezb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SH9iX7TiQEwM"},"execution_count":null,"outputs":[]}]}