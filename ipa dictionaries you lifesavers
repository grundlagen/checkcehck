{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true,"history_visible":true,"authorship_tag":"ABX9TyNndxhLy2/FOevMPnkFYPUw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import logging\n","\n","# Configure logging\n","logging.basicConfig(filename=\"conversion_errors.log\", level=logging.ERROR)\n","\n","# Load the CSV files\n","en_uk_df = pd.read_csv('/content/en_UK.csv')\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Display a sample of the data\n","print(\"Sample from English (UK) dictionary:\")\n","print(en_uk_df.head())\n","\n","print(\"Sample from English (US) dictionary:\")\n","print(en_us_df.head())\n","\n","print(\"Sample from French dictionary:\")\n","print(fr_df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGwsQXkntjRY","executionInfo":{"status":"ok","timestamp":1719609476853,"user_tz":-60,"elapsed":871,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"7edbec1b-3ddd-43a4-fb2d-a2905e6c32b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample from English (UK) dictionary:\n","         aah        /ˈɑː/\n","0   aardvark   /ˈɑːdvɑːk/\n","1  aardvarks  /ˈɑːdvɑːks/\n","2   aardwolf   /ˈɑːdwʊlf/\n","3        aba       /ɐbˈæ/\n","4      abaca     /æbˈækɐ/\n","Sample from English (US) dictionary:\n","     'bout     /ˈbaʊt/\n","0   'cause       /kəz/\n","1  'course     /ˈkɔɹs/\n","2    'cuse     /ˈkjuz/\n","3      'em        /əm/\n","4  'frisco  /ˈfɹɪskoʊ/\n","Sample from French dictionary:\n","                a           /a/\n","0               A           /a/\n","1               à           /a/\n","2  à aucun moment  /aokœ̃mɔmɑ̃/\n","3    à aucun prix    /aokœ̃pʁi/\n","4   à aucun titre   /aokœ̃titʁ/\n"]}]},{"cell_type":"code","source":["# Function to ensure a minimum number of words\n","def ensure_min_words(df, min_words=8000):\n","    if len(df) < min_words:\n","        logging.error(f\"Dictionary has less than {min_words} words.\")\n","    return df\n","\n","# Ensure minimum word count\n","min_words = 8000\n","en_uk_df = ensure_min_words(en_uk_df, min_words)\n","en_us_df = ensure_min_words(en_us_df, min_words)\n","fr_df = ensure_min_words(fr_df, min_words)\n","\n","print(f\"English (UK) dictionary words: {len(en_uk_df)}\")\n","print(f\"English (US) dictionary words: {len(en_us_df)}\")\n","print(f\"French dictionary words: {len(fr_df)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGn_uXp0tm0d","executionInfo":{"status":"ok","timestamp":1719609498827,"user_tz":-60,"elapsed":409,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"4a023989-da30-4381-f832-d9971672acdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English (UK) dictionary words: 65117\n","English (US) dictionary words: 125926\n","French dictionary words: 245177\n"]}]},{"cell_type":"code","source":["import os\n","os.environ['DASK_DISTRIBUTED__WORKER__RESOURCES__MEMORY_LIMIT'] = '35GB'\n"],"metadata":{"id":"jmnzFon4JcUb","executionInfo":{"status":"ok","timestamp":1719650342267,"user_tz":-60,"elapsed":255,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Ensure the columns are appropriately named\n","en_uk_df.columns = [\"word_en\", \"ipa\"]\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Function to find similar IPA transcriptions\n","def find_similar_ipa(df1, df2):\n","    matches = []\n","    for i, row1 in df1.iterrows():\n","        for j, row2 in df2.iterrows():\n","            if row1['ipa'] == row2['ipa']:\n","                matches.append((row1['word_en'], row2['word_fr'], row1['ipa']))\n","    return matches\n","\n","# Combine English dictionaries\n","en_df = pd.concat([en_uk_df, en_us_df]).drop_duplicates().reset_index(drop=True)\n","\n","# Find similar IPA transcriptions\n","similar_ipa_matches = find_similar_ipa(en_df, fr_df)\n","\n","# Convert to DataFrame\n","similar_ipa_df = pd.DataFrame(similar_ipa_matches, columns=['word_en', 'word_fr', 'ipa'])\n","\n","# Save similar IPA matches to CSV\n","similar_ipa_df.to_csv(\"similar_ipa_homophones.csv\", index=False)\n","\n","print(\"IPA similarity matching completed.\")\n","print(\"Sample from IPA matched homophones:\")\n","print(similar_ipa_df.head())\n"],"metadata":{"id":"xQWBrakztrUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import unicodedata\n","import logging\n","!pip install textdistance\n","!pip install tqdm import tqdm\n","import time\n","!pip install rapidfuzz import process, fuzz\n","from multiprocessing import Pool\n","import numpy as np\n","import cProfile, pstats\n","import os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTiqtwu-C8vI","executionInfo":{"status":"ok","timestamp":1719648713203,"user_tz":-60,"elapsed":9938,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"642f2329-79b5-4fc3-b8e4-90ff1f5e0172"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textdistance\n","  Downloading textdistance-4.6.2-py3-none-any.whl (31 kB)\n","Installing collected packages: textdistance\n","Successfully installed textdistance-4.6.2\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement import (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for import\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Invalid requirement: 'process,'\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install rapidfuzz pandas unicodedata textdistance tqdm dask[complete]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOgkwF43Dbf4","executionInfo":{"status":"ok","timestamp":1719648770097,"user_tz":-60,"elapsed":5206,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"5851ef84-e169-4aaf-b986-1c5eb8518ca6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rapidfuzz\n","  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for unicodedata\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rNqw4zrmEGGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rapidfuzz\n","import pandas as pd\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import time\n","from rapidfuzz import process, fuzz\n","from multiprocessing import Pool\n","import numpy as np\n","import cProfile, pstats\n","import os\n","import unicodedata\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","# Load and normalize your homophone list\n","homophones = [\n","    (\"une ouef\", \"enough\"),\n","    (\"seau\", \"so\"),\n","    # ... (rest of your homophones)\n","]\n","\n","en_homophones = [pair[1] for pair in homophones]\n","\n","# -- OPTIMIZED FUNCTIONS ---\n","\n","def find_similar_ipa_chunk(df1_chunk, df2, threshold):\n","    ipa_set_fr = set(df2['ipa'])\n","    matches = []\n","    logging.info(f\"Process {os.getpid()} started processing chunk of size {len(df1_chunk)}\")\n","    for ipa1 in df1_chunk:\n","        for ipa2 in ipa_set_fr:\n","            similarity = textdistance.jaro_winkler.normalized_similarity(ipa1, ipa2)\n","            if similarity >= threshold:\n","                word_en = df1.loc[df1['ipa'] == ipa1, 'word_en'].iloc[0]\n","                word_fr = df2.loc[df2['ipa'] == ipa2, 'word_fr'].iloc[0]\n","                matches.append((word_en, word_fr, ipa1, ipa2, similarity))\n","    logging.info(f\"Process {os.getpid()} finished processing chunk\")  # Log completion\n","    return matches\n","\n","def find_similar_ipa_parallel(df1, df2, threshold=0.9, num_processes=4):\n","    if 'ipa' not in df1.columns or 'ipa' not in df2.columns:\n","        raise ValueError(\"Both DataFrames must have an 'ipa' column.\")\n","    error_count = 0\n","    max_errors = 3\n","    while True:\n","        try:\n","            profiler = cProfile.Profile()  # Initialize the profiler\n","            profiler.enable()              # Start profiling\n","            with Pool(processes=num_processes) as pool:\n","                chunk_size = len(df1) // num_processes\n","                chunks = np.array_split(df1['ipa'], num_processes)\n","                results = pool.starmap(find_similar_ipa_chunk, [(chunk, df2, threshold) for chunk in chunks])\n","            profiler.disable()\n","            stats = pstats.Stats(profiler).sort_stats('cumtime')\n","            stats.print_stats(20)\n","\n","            return [match for sublist in results for match in sublist]\n","\n","        except Exception as e:\n","            logging.error(f\"Error in multiprocessing: {e}\")\n","            error_count += 1\n","            if error_count >= max_errors:\n","                logging.warning(\"Falling back to Dask due to repeated multiprocessing errors.\")\n","                return find_similar_ipa_dask(df1, df2, threshold)  # Fallback to Dask\n","            time.sleep(5)\n","\n","\n","def find_similar_ipa_dask(df1, df2, threshold=0.9):\n","    import dask.dataframe as dd\n","\n","    # Convert Pandas Dataframes to Dask Dataframes\n","    ddf1 = dd.from_pandas(df1, npartitions=4)  # Adjust npartitions based on your system\n","    ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","    # Cartesian product to get all combinations of IPA pairs\n","    ddf_merged = ddf1.merge(ddf2, how='cross')\n","\n","    # Calculate Jaro-Winkler similarity and filter\n","    ddf_merged['similarity'] = ddf_merged.apply(lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_x'], row['ipa_y']), axis=1, meta=('similarity', 'f8'))\n","    ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","    # Convert back to Pandas DataFrame and return\n","    return ddf_filtered[['word_en', 'word_fr', 'ipa_x', 'ipa_y', 'similarity']].compute().rename(columns={'ipa_x':'ipa_en', 'ipa_y':'ipa_fr'})\n","\n","# --- MAIN EXECUTION ---\n","\n","# Filter en_us_df based on homophones\n","en_us_df_filtered = en_us_df[en_us_df['word_en'].isin(en_homophones)]\n","similar_ipa_matches = find_similar_ipa_parallel(en_us_df_filtered, fr_df)\n","\n","# ... (Rest of your code to create DataFrame and save results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UB8cxe4FD58T","executionInfo":{"status":"ok","timestamp":1719649049186,"user_tz":-60,"elapsed":14064,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"032f3f18-4846-4020-e68c-0666d14ba6f6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rapidfuzz\n","  Using cached rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","Installing collected packages: rapidfuzz\n","Successfully installed rapidfuzz-3.9.3\n","         4293 function calls (4270 primitive calls) in 3.101 seconds\n","\n","   Ordered by: cumulative time\n","   List reduced from 414 to 20 due to restriction <20>\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","       12    0.000    0.000    2.960    0.247 /usr/lib/python3.10/threading.py:589(wait)\n","       12    0.000    0.000    2.960    0.247 /usr/lib/python3.10/threading.py:288(wait)\n","       75    2.960    0.039    2.960    0.039 {method 'acquire' of '_thread.lock' objects}\n","        1    0.000    0.000    2.950    2.950 /usr/lib/python3.10/multiprocessing/pool.py:369(starmap)\n","        1    0.000    0.000    2.950    2.950 /usr/lib/python3.10/multiprocessing/pool.py:767(get)\n","        1    0.000    0.000    2.950    2.950 /usr/lib/python3.10/multiprocessing/pool.py:764(wait)\n","        1    0.000    0.000    0.128    0.128 /usr/lib/python3.10/multiprocessing/context.py:115(Pool)\n","        1    0.000    0.000    0.124    0.124 /usr/lib/python3.10/multiprocessing/pool.py:183(__init__)\n","        1    0.000    0.000    0.119    0.119 /usr/lib/python3.10/multiprocessing/pool.py:305(_repopulate_pool)\n","        1    0.000    0.000    0.119    0.119 /usr/lib/python3.10/multiprocessing/pool.py:314(_repopulate_pool_static)\n","        4    0.000    0.000    0.118    0.029 /usr/lib/python3.10/multiprocessing/process.py:110(start)\n","        4    0.000    0.000    0.117    0.029 /usr/lib/python3.10/multiprocessing/context.py:278(_Popen)\n","        4    0.000    0.000    0.116    0.029 /usr/lib/python3.10/multiprocessing/popen_fork.py:15(__init__)\n","        4    0.000    0.000    0.094    0.023 /usr/lib/python3.10/multiprocessing/util.py:433(_flush_std_streams)\n","        8    0.000    0.000    0.094    0.012 /usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py:335(flush)\n","       16    0.000    0.000    0.084    0.005 /usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py:195(schedule)\n","       16    0.084    0.005    0.084    0.005 /usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py:545(send)\n","        4    0.001    0.000    0.022    0.006 /usr/lib/python3.10/multiprocessing/popen_fork.py:62(_launch)\n","        4    0.021    0.005    0.021    0.005 {built-in method posix.fork}\n","        1    0.000    0.000    0.019    0.019 /usr/lib/python3.10/multiprocessing/pool.py:738(__exit__)\n","\n","\n"]}]},{"cell_type":"code","source":["!pip install rapidfuzz pandas unicodedata textdistance tqdm dask[complete]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GI0q3JIiFX6Z","executionInfo":{"status":"ok","timestamp":1719649275564,"user_tz":-60,"elapsed":1248,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"e9dcfaaf-7441-4419-d052-36f866e1dcd1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (3.9.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for unicodedata\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Install Modin\n","!pip install \"modin[all]\" --force-reinstall\n","!pip uninstall pandas -y\n","!pip install pandas==2.2.2\n","\n","import pandas as pd\n","import unicodedata\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import time\n","from rapidfuzz import process, fuzz\n","import dask.dataframe as dd\n","import os\n","import modin.pandas as mpd\n","\n","# ... (rest of your code - normalization, dataframe creation, etc.)\n","\n","\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","# Load and normalize your homophone list\n","homophones = [\n","    (\"une ouef\", \"enough\"),\n","    (\"seau\", \"so\"),\n","    # ... (rest of your homophones)\n","]\n","\n","en_homophones = [pair[1] for pair in homophones]\n","\n","# -- DASK IMPLEMENTATION (CORRECTED) ---\n","\n","def find_similar_ipa_dask(df1, df2, threshold=0.9):\n","    ddf1 = dd.from_pandas(df1, npartitions=4)\n","    ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","    # Rename columns to avoid conflicts\n","    ddf1 = ddf1.rename(columns={'ipa': 'ipa_en'})\n","    ddf2 = ddf2.rename(columns={'ipa': 'ipa_fr'})\n","\n","    # Merge on a dummy column to get all combinations (similar to cross join)\n","    ddf1['key'] = 1\n","    ddf2['key'] = 1\n","    ddf_merged = dd.merge(ddf1, ddf2, on='key', suffixes=('', '_'))\n","\n","    # Calculate Jaro-Winkler similarity and filter\n","    ddf_merged['similarity'] = ddf_merged.apply(lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_en'], row['ipa_fr']), axis=1, meta=('similarity', 'f8'))\n","    ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","    # Convert back to Pandas DataFrame and return\n","    return ddf_filtered[['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity']].compute()\n","\n","\n","# --- MAIN EXECUTION ---\n","\n","# Filter en_us_df based on homophones\n","en_us_df_filtered = en_us_df[en_us_df['word_en'].isin(en_homophones)]\n","similar_ipa_matches = find_similar_ipa_dask(en_us_df_filtered, fr_df)\n","\n","# ... (Create DataFrame and save results as before)\n"],"metadata":{"id":"oK2D1aoGGGlt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y6ebLOoBOLGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import unicodedata\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import time\n","from rapidfuzz import process, fuzz\n","import dask.dataframe as dd\n","\n","\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","# Load and normalize your homophone list\n","homophones = [\n","    (\"une ouef\", \"enough\"),\n","    (\"seau\", \"so\"),\n","    # ... (rest of your homophones)\n","]\n","\n","en_homophones = [pair[1] for pair in homophones]\n","\n","# -- DASK IMPLEMENTATION (CORRECTED) ---\n","\n","def find_similar_ipa_dask(df1, df2, threshold=0.9):\n","    ddf1 = dd.from_pandas(df1, npartitions=4)\n","    ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","    # Rename columns to avoid conflicts\n","    ddf1 = ddf1.rename(columns={'ipa': 'ipa_en'})\n","    ddf2 = ddf2.rename(columns={'ipa': 'ipa_fr'})\n","\n","    # Merge on a dummy column to get all combinations (similar to cross join)\n","    ddf1['key'] = 1\n","    ddf2['key'] = 1\n","    ddf_merged = dd.merge(ddf1, ddf2, on='key', suffixes=('', '_'))\n","\n","    # Calculate Jaro-Winkler similarity and filter\n","    ddf_merged['similarity'] = ddf_merged.apply(lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_en'], row['ipa_fr']), axis=1, meta=('similarity', 'f8'))\n","    ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","    # Convert back to Pandas DataFrame and return\n","    return ddf_filtered[['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity']].compute()\n","\n","\n","# --- MAIN EXECUTION ---\n","\n","# Filter en_us_df based on homophones\n","en_us_df_filtered = en_us_df[en_us_df['word_en'].isin(en_homophones)]\n","similar_ipa_matches = find_similar_ipa_dask(en_us_df_filtered, fr_df)\n","\n","# Create DataFrame, handle empty results\n","if similar_ipa_matches.empty:\n","    logging.info(\"No matches found.\")\n","    similar_ipa_df = pd.DataFrame(columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])\n","else:\n","    similar_ipa_df = similar_ipa_matches\n","\n","# Save results to CSV\n","output_filename = \"similar_ipa_homophones.csv\"\n","similar_ipa_df.to_csv(output_filename, index=False)\n","\n","print(f\"IPA similarity matching completed. Results saved to {output_filename}\")\n","print(\"Sample from IPA matched homophones:\")\n","print(similar_ipa_df.head(20))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCenSffpGs7q","executionInfo":{"status":"ok","timestamp":1719649637000,"user_tz":-60,"elapsed":9302,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"f0e11e9a-00da-467b-eb44-a73023de5bd4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["IPA similarity matching completed. Results saved to similar_ipa_homophones.csv\n","Sample from IPA matched homophones:\n","Empty DataFrame\n","Columns: [word_en, word_fr, ipa_en, ipa_fr, similarity]\n","Index: []\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import unicodedata\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import time\n","from rapidfuzz import process, fuzz\n","import dask.dataframe as dd\n","\n","\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","\n","# --- OPTIMIZED FUNCTIONS ---\n","\n","def find_similar_ipa_dask(df1, df2, threshold=0.87):\n","    ddf1 = dd.from_pandas(df1, npartitions=4)\n","    ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","    # Pre-filtering based on shared characters\n","    def count_shared_chars(ipa1, ipa2):\n","        return sum(1 for c in ipa1 if c in ipa2)\n","\n","    ddf1['shared_chars'] = ddf1['ipa'].apply(lambda ipa1: ddf2['ipa'].apply(lambda ipa2: count_shared_chars(ipa1, ipa2)).max(), meta=('shared_chars', 'i8'))\n","    ddf1 = ddf1[ddf1['shared_chars'] >= (max(len(ipa) for ipa in ddf1['ipa']) + max(len(ipa) for ipa in ddf2['ipa'])) // 3]  # At least 1/3 shared characters\n","\n","    # Rename columns to avoid conflicts\n","    ddf1 = ddf1.rename(columns={'ipa': 'ipa_en'})\n","    ddf2 = ddf2.rename(columns={'ipa': 'ipa_fr'})\n","\n","    # Merge on a dummy column to get all combinations (similar to cross join)\n","    ddf1['key'] = 1\n","    ddf2['key'] = 1\n","    ddf_merged = dd.merge(ddf1, ddf2, on='key', suffixes=('', '_'))\n","\n","    # Calculate Jaro-Winkler similarity and filter\n","    ddf_merged['similarity'] = ddf_merged.apply(lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_en'], row['ipa_fr']), axis=1, meta=('similarity', 'f8'))\n","    ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","    # Convert back to Pandas DataFrame and return\n","    return ddf_filtered[['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity']].compute()\n","\n","\n","# --- MAIN EXECUTION ---\n","\n","similar_ipa_matches = find_similar_ipa_dask(en_us_df, fr_df)\n","\n","# Create DataFrame, handle empty results\n","if similar_ipa_matches.empty:\n","    logging.info(\"No matches found.\")\n","    similar_ipa_df = pd.DataFrame(columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])\n","else:\n","    similar_ipa_df = similar_ipa_matches\n","\n","# Save results to CSV\n","output_filename = \"similar_ipa_homophones.csv\"\n","similar_ipa_df.to_csv(output_filename, index=False)\n","\n","print(f\"IPA similarity matching completed. Results saved to {output_filename}\")\n","print(\"Sample from IPA matched homophones:\")\n","print(similar_ipa_df.head(20))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8B9DjbfjHZjv","executionInfo":{"status":"error","timestamp":1719649807349,"user_tz":-60,"elapsed":1933,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"17fa5ea6-8938-428a-e67a-96c0f7dcc97d"},"execution_count":16,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Metadata inference failed in `apply`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nTypeError(\"argument of type 'NAType' is not iterable\")\n\nTraceback:\n---------\n  File \"/usr/local/lib/python3.10/dist-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/usr/local/lib/python3.10/dist-packages/dask/dataframe/core.py\", line 6893, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/usr/local/lib/python3.10/dist-packages/dask/utils.py\", line 1105, in __call__\n    return getattr(__obj, self.method)(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\", line 4630, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\", line 1025, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\", line 1076, in apply_standard\n    mapped = lib.map_infer(\n  File \"pandas/_libs/lib.pyx\", line 2834, in pandas._libs.lib.map_infer\n  File \"<ipython-input-16-adf662cf2238>\", line 46, in <lambda>\n    ddf1['shared_chars'] = ddf1['ipa'].apply(lambda ipa1: ddf2['ipa'].apply(lambda ipa2: count_shared_chars(ipa1, ipa2)).max(), meta=('shared_chars', 'i8'))\n  File \"<ipython-input-16-adf662cf2238>\", line 44, in count_shared_chars\n    return sum(1 for c in ipa1 if c in ipa2)\n  File \"<ipython-input-16-adf662cf2238>\", line 44, in <genexpr>\n    return sum(1 for c in ipa1 if c in ipa2)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/dataframe/utils.py\u001b[0m in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6892\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mraise_on_meta_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuncname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_numeric_only_deprecation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_extract_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_extract_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, _methodcaller__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \"\"\"\n\u001b[0;32m-> 4630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(ipa2)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_shared_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mddf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# At least 1/3 shared characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36mcount_shared_chars\u001b[0;34m(ipa1, ipa2)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_shared_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipa1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_shared_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipa1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: argument of type 'NAType' is not iterable","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# --- MAIN EXECUTION ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0msimilar_ipa_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_similar_ipa_dask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_us_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Create DataFrame, handle empty results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36mfind_similar_ipa_dask\u001b[0;34m(df1, df2, threshold)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_shared_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mddf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# At least 1/3 shared characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Rename columns to avoid conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_shared_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mddf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# At least 1/3 shared characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Rename columns to avoid conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiprocessingPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m                             \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Re-execute locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                             \u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cache\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-16-adf662cf2238>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(ipa1)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipa1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_shared_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipa2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mddf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_chars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mipa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ipa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# At least 1/3 shared characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/dataframe/utils.py\u001b[0m in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" in `{funcname}`\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfuncname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Metadata inference failed in `apply`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nTypeError(\"argument of type 'NAType' is not iterable\")\n\nTraceback:\n---------\n  File \"/usr/local/lib/python3.10/dist-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/usr/local/lib/python3.10/dist-packages/dask/dataframe/core.py\", line 6893, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/usr/local/lib/python3.10/dist-packages/dask/utils.py\", line 1105, in __call__\n    return getattr(__obj, self.method)(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\", line 4630, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\", line 1025, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\", line 1076, in apply_standard\n    mapped = lib.map_infer(\n  File \"pandas/_libs/lib.pyx\", line 2834, in pandas._libs.lib.map_infer\n  File \"<ipython-input-16-adf662cf2238>\", line 46, in <lambda>\n    ddf1['shared_chars'] = ddf1['ipa'].apply(lambda ipa1: ddf2['ipa'].apply(lambda ipa2: count_shared_chars(ipa1, ipa2)).max(), meta=('shared_chars', 'i8'))\n  File \"<ipython-input-16-adf662cf2238>\", line 44, in count_shared_chars\n    return sum(1 for c in ipa1 if c in ipa2)\n  File \"<ipython-input-16-adf662cf2238>\", line 44, in <genexpr>\n    return sum(1 for c in ipa1 if c in ipa2)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import unicodedata\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import time\n","from rapidfuzz import process, fuzz\n","import dask.dataframe as dd\n","\n","\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","\n","# --- OPTIMIZED FUNCTIONS (DASK WITH FILTERING) ---\n","\n","def find_similar_ipa_dask(df1, df2, threshold=0.87):\n","    ddf1 = dd.from_pandas(df1, npartitions=4)\n","    ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","    # Pre-filtering based on shared characters (handling missing values)\n","    def count_shared_chars(ipa1, ipa2):\n","        if pd.isnull(ipa1) or pd.isnull(ipa2):\n","            return 0\n","        return sum(1 for c in ipa1 if c in ipa2)\n","\n","    # Rename columns to avoid conflicts\n","    ddf1 = ddf1.rename(columns={'ipa': 'ipa_en'})\n","    ddf2 = ddf2.rename(columns={'ipa': 'ipa_fr'})\n","\n","    # Merge on a dummy column to get all combinations (similar to cross join)\n","    ddf1['key'] = 1\n","    ddf2['key'] = 1\n","    ddf_merged = dd.merge(ddf1, ddf2, on='key', suffixes=('', '_'))\n","    ddf_merged = ddf_merged.drop('key', axis=1)\n","\n","    # Calculate required shared characters for each pair\n","    ddf_merged['required_shared_chars'] = (\n","        ddf_merged[['ipa_en', 'ipa_fr']].apply(\n","            lambda row: (max(len(row['ipa_en']), len(row['ipa_fr'])) // 3) + 1,\n","            axis=1,\n","            meta=('required_shared_chars', 'i8')\n","        )\n","    )\n","\n","    # Count shared characters and filter\n","    ddf_merged['shared_chars'] = ddf_merged.apply(\n","        lambda row: count_shared_chars(row['ipa_en'], row['ipa_fr']),\n","        axis=1,\n","        meta=('shared_chars', 'i8')\n","    )\n","    ddf_merged = ddf_merged[ddf_merged['shared_chars'] >= ddf_merged['required_shared_chars']]\n","\n","    # Calculate Jaro-Winkler similarity and filter\n","    ddf_merged['similarity'] = ddf_merged.apply(\n","        lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_en'], row['ipa_fr']) if pd.notnull(row['ipa_en']) and pd.notnull(row['ipa_fr']) else 0,\n","        axis=1,\n","        meta=('similarity', 'f8')\n","    )\n","    ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","    # Convert back to Pandas DataFrame and return\n","    return ddf_filtered[['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity']].compute()\n","\n","# --- MAIN EXECUTION ---\n","\n","similar_ipa_matches = find_similar_ipa_dask(en_us_df, fr_df)\n","\n","# Create DataFrame, handle empty results\n","if similar_ipa_matches.empty:\n","    logging.info(\"No matches found.\")\n","    similar_ipa_df = pd.DataFrame(columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])\n","else:\n","    similar_ipa_df = similar_ipa_matches\n","\n","# Save results to CSV\n","output_filename = \"similar_ipa_homophones.csv\"\n","similar_ipa_df.to_csv(output_filename, index=False)\n","\n","print(f\"IPA similarity matching completed. Results saved to {output_filename}\")\n","print(\"Sample from IPA matched homophones:\")\n","print(similar_ipa_df.head(20))\n"],"metadata":{"id":"OQlYf22RID61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import unicodedata\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import dask.dataframe as dd\n","import os\n","\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","\n","# --- OPTIMIZED FUNCTIONS (DASK WITH FILTERING AND CHUNKING) ---\n","\n","def find_similar_ipa_dask_chunk(df1_chunk, df2, threshold=0.87, max_len_diff=3):\n","    ddf1 = dd.from_pandas(df1_chunk, npartitions=4)\n","    ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","    # Aggressive pre-filtering based on shared characters and length difference\n","    def count_shared_chars(ipa1, ipa2):\n","        if pd.isnull(ipa1) or pd.isnull(ipa2):\n","            return 0\n","        return sum(1 for c in ipa1 if c in ipa2)\n","\n","    # Rename columns to avoid conflicts\n","    ddf1 = ddf1.rename(columns={'ipa': 'ipa_en'})\n","    ddf2 = ddf2.rename(columns={'ipa': 'ipa_fr'})\n","\n","    # Merge on a dummy column to get all combinations (similar to cross join)\n","    ddf1['key'] = 1\n","    ddf2['key'] = 1\n","    ddf_merged = dd.merge(ddf1, ddf2, on='key', suffixes=('', '_'))\n","    ddf_merged = ddf_merged.drop('key', axis=1)\n","\n","    # Pre-filtering: length difference\n","    ddf_merged['len_diff'] = ddf_merged.apply(lambda row: abs(len(row['ipa_en']) - len(row['ipa_fr'])), axis=1, meta=('len_diff', 'i8'))\n","    ddf_merged = ddf_merged[ddf_merged['len_diff'] <= max_len_diff]  # Filter out pairs with large length differences\n","\n","    # Pre-filtering: shared characters\n","    ddf_merged['required_shared_chars'] = (\n","        ddf_merged[['ipa_en', 'ipa_fr']].apply(\n","            lambda row: (max(len(row['ipa_en']), len(row['ipa_fr'])) // 3) + 1,\n","            axis=1,\n","            meta=('required_shared_chars', 'i8')\n","        )\n","    )\n","    ddf_merged['shared_chars'] = ddf_merged.apply(lambda row: count_shared_chars(row['ipa_en'], row['ipa_fr']), axis=1, meta=('shared_chars', 'i8'))\n","    ddf_merged = ddf_merged[ddf_merged['shared_chars'] >= ddf_merged['required_shared_chars']]\n","\n","    # Calculate Jaro-Winkler similarity and filter\n","    ddf_merged['similarity'] = ddf_merged.apply(\n","        lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_en'], row['ipa_fr']) if pd.notnull(row['ipa_en']) and pd.notnull(row['ipa_fr']) else 0,\n","        axis=1,\n","        meta=('similarity', 'f8')\n","    )\n","    ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","    # Convert back to Pandas DataFrame and return\n","    return ddf_filtered[['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity']].compute()\n","\n","\n","def find_similar_ipa_dask_chunked(df1, df2, threshold=0.87, chunk_size=10000, max_len_diff=3):\n","    similar_ipa_matches = []\n","    total_chunks = (len(df1) - 1) // chunk_size + 1  # Calculate total chunks\n","    with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:  # Use tqdm for progress bar\n","        for i in range(0, len(df1), chunk_size):\n","            df1_chunk = df1[i:i + chunk_size]\n","            chunk_matches = find_similar_ipa_dask_chunk(df1_chunk, df2, threshold, max_len_diff)\n","            similar_ipa_matches.extend(chunk_matches)\n","            pbar.update(1)  # Update progress bar\n","    return pd.DataFrame(similar_ipa_matches, columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])\n","\n","\n","# --- MAIN EXECUTION ---\n","# Set Dask memory limit (experimental, may not be strictly enforced)\n","os.environ['DASK_DISTRIBUTED__WORKER__RESOURCES__MEMORY_LIMIT'] = '35GB'\n","\n","\n","similar_ipa_matches = find_similar_ipa_dask_chunked(en_us_df, fr_df)  # No filtering on en_us_df\n","\n","# Create DataFrame, handle empty results\n","if similar_ipa_matches.empty:\n","    logging.info(\"No matches found.\")\n","    similar_ipa_df = pd.DataFrame(columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])\n","else:\n","    similar_ipa_df = similar_ipa_matches\n","\n","# Save results to CSV\n","output_filename = \"similar_ipa_homophones.csv\"\n","similar_ipa_df.to_csv(output_filename, index=False)\n","\n","print(f\"IPA similarity matching completed. Results saved to {output_filename}\")\n","print(\"Sample from IPA matched homophones:\")\n","print(similar_ipa_df.head(20))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61FqAMVdJmWc","outputId":"50c8378c-a8dc-4bcd-b6bb-b73b97abf403"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rProcessing chunks:   0%|          | 0/13 [00:00<?, ?it/s]"]}]},{"cell_type":"code","source":["\n","import unicodedata\n","import logging\n","import textdistance\n","from tqdm import tqdm\n","import time\n","from rapidfuzz import process, fuzz\n","import dask.dataframe as dd\n","import os\n","!pip install modin[all]\n","import modin.pandas as mpd\n","import modin.pandas as mpd  # For Modin (optional)\n","# Configure logging\n","logging.basicConfig(filename=\"process_log.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Load the CSV files using Modin (optional - for faster processing)\n","# Uncomment these lines to use Modin:\n","# en_us_df = mpd.read_csv('/content/en_US.csv')\n","# fr_df = mpd.read_csv('/content/fr_FR.csv')\n","en_us_df = pd.read_csv('/content/en_US.csv')\n","fr_df = pd.read_csv('/content/fr_FR.csv')\n","\n","# Function to normalize IPA transcriptions\n","def normalize_ipa(df, column):\n","    df[column] = df[column].apply(lambda x: unicodedata.normalize('NFC', str(x).strip()) if pd.notnull(x) else x)\n","    return df\n","\n","# Ensure the columns are appropriately named\n","en_us_df.columns = [\"word_en\", \"ipa\"]\n","fr_df.columns = [\"word_fr\", \"ipa\"]\n","\n","# Normalize IPA transcriptions\n","en_us_df = normalize_ipa(en_us_df, 'ipa')\n","fr_df = normalize_ipa(fr_df, 'ipa')\n","\n","# Remove duplicates within each dataset\n","en_us_df = en_us_df.drop_duplicates().reset_index(drop=True)\n","fr_df = fr_df.drop_duplicates().reset_index(drop=True)\n","\n","# --- OPTIMIZED FUNCTIONS (DASK WITH AGGRESSIVE FILTERING AND CHUNKING) ---\n","\n","def find_similar_ipa_dask_chunk(df1_chunk, df2, threshold=0.87, max_len_diff=3):\n","    try:\n","        ddf1 = dd.from_pandas(df1_chunk, npartitions=4)\n","        ddf2 = dd.from_pandas(df2, npartitions=4)\n","\n","        # Aggressive pre-filtering\n","        # ... (Same filtering logic as before: shared chars, length diff, vowel count if needed)\n","\n","        # Calculate Jaro-Winkler similarity and filter\n","        ddf_merged['similarity'] = ddf_merged.apply(\n","            lambda row: textdistance.jaro_winkler.normalized_similarity(row['ipa_en'], row['ipa_fr']) if pd.notnull(row['ipa_en']) and pd.notnull(row['ipa_fr']) else 0,\n","            axis=1,\n","            meta=('similarity', 'f8')\n","        )\n","        ddf_filtered = ddf_merged[ddf_merged['similarity'] >= threshold]\n","\n","        # Convert back to Pandas DataFrame and return\n","        return ddf_filtered[['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity']].compute()\n","\n","    except Exception as e:  # Catch any errors within the chunk processing\n","        logging.error(f\"Error processing chunk: {e}\")\n","        return pd.DataFrame(columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])  # Return an empty DataFrame if there's an error\n","\n","\n","def find_similar_ipa_dask_chunked(df1, df2, threshold=0.87, chunk_size=1000, max_len_diff=3): # Reduced chunk size\n","    similar_ipa_matches = []\n","    total_chunks = (len(df1) - 1) // chunk_size + 1\n","    with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n","        for i in range(0, len(df1), chunk_size):\n","            df1_chunk = df1[i:i + chunk_size]\n","            chunk_matches = find_similar_ipa_dask_chunk(df1_chunk, df2, threshold, max_len_diff)\n","            similar_ipa_matches.append(chunk_matches)\n","            pbar.update(1)\n","    return pd.concat(similar_ipa_matches, ignore_index=True)  # Concatenate DataFrames\n","\n","# --- MAIN EXECUTION ---\n","\n","# Set Dask memory limit (experimental, may not be strictly enforced)\n","os.environ['DASK_DISTRIBUTED__WORKER__RESOURCES__MEMORY_LIMIT'] = '35GB'\n","\n","\n","similar_ipa_matches = find_similar_ipa_dask_chunked(en_us_df, fr_df) # No filtering on en_us_df\n","\n","# Create DataFrame, handle empty results\n","if similar_ipa_matches.empty:\n","    logging.info(\"No matches found.\")\n","    similar_ipa_df = pd.DataFrame(columns=['word_en', 'word_fr', 'ipa_en', 'ipa_fr', 'similarity'])\n","else:\n","    similar_ipa_df = similar_ipa_matches\n","\n","# Save results to CSV\n","output_filename = \"similar_ipa_homophones.csv\"\n","similar_ipa_df.to_csv(output_filename, index=False)\n","\n","print(f\"IPA similarity matching completed. Results saved to {output_filename}\")\n","print(\"Sample from IPA matched homophones:\")\n","print(similar_ipa_df.head(20))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eDXD4FlvKyTJ","executionInfo":{"status":"error","timestamp":1719651193316,"user_tz":-60,"elapsed":14227,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"66b4bfbc-9109-4457-a513-5122c70836a8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: modin[all] in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Collecting pandas<2.3,>=2.2 (from modin[all])\n","  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (24.1)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (1.25.2)\n","Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2023.6.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (5.9.5)\n","Requirement already satisfied: dask>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2023.8.1)\n","Requirement already satisfied: distributed>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2023.8.1)\n","Requirement already satisfied: ray!=2.5.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2.31.0)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (14.0.2)\n","Requirement already satisfied: modin-spreadsheet>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (0.1.2)\n","Requirement already satisfied: dataframe-api-compat>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (0.2.7)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (8.1.7)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (2.2.1)\n","Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (6.0.1)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (0.12.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (7.2.0)\n","Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (3.1.4)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (1.0.8)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (3.0.0)\n","Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (6.3.3)\n","Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (2.0.7)\n","Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (3.0.0)\n","Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from modin-spreadsheet>=0.1.0->modin[all]) (1.0.0)\n","Requirement already satisfied: notebook>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from modin-spreadsheet>=0.1.0->modin[all]) (6.5.5)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from modin-spreadsheet>=0.1.0->modin[all]) (7.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[all]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[all]) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[all]) (2024.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (3.15.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (4.19.2)\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (3.20.3)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (1.3.1)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (2.31.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2.22.0->modin[all]) (3.19.2)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (5.5.6)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (3.6.6)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (7.34.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (3.0.11)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2.22.0->modin[all]) (2.1.5)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (5.5.2)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (6.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (6.5.4)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (23.1.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (5.7.2)\n","Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (6.1.12)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (5.10.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.20.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->modin[all]) (1.16.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (0.18.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[all]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[all]) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[all]) (2024.6.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (67.7.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (3.0.47)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.9.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (4.2.2)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.2.4)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (1.3.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (2.20.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (21.2.0)\n","Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (2.4.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.8.4)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.24.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.2.13)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (2.22)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.8.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.2.1)\n","Installing collected packages: pandas\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pandas-2.2.2\n"]},{"output_type":"stream","name":"stderr","text":["UserWarning: The pandas version installed (2.0.3) does not match the supported pandas version in Modin (2.2.X). This may cause undesired side effects!\n"]},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'SparseDtype' from 'pandas.core.dtypes.dtypes' (/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/dtypes.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-c3c637d955bc>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install modin[all]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpd\u001b[0m  \u001b[0;31m# For Modin (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Configure logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/modin/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m from .general import (\n\u001b[1;32m    146\u001b[0m     \u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/modin/pandas/dataframe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCachedAccessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseFrameAccessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ATTRS_NO_LOOKUP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasePandasDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/modin/pandas/accessor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompressionOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStorageOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'SparseDtype' from 'pandas.core.dtypes.dtypes' (/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/dtypes.py)"]}]},{"cell_type":"code","source":["!pip uninstall pandas -y\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doIVnl6TMUFB","executionInfo":{"status":"ok","timestamp":1719651173482,"user_tz":-60,"elapsed":1037,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"5bc71d2f-77d1-4696-e191-5c195a5d6eb8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: pandas 2.2.2\n","Uninstalling pandas-2.2.2:\n","  Successfully uninstalled pandas-2.2.2\n"]}]},{"cell_type":"code","source":["!pip install \"modin[all]\"  # This will install the compatible Pandas version automatically\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5EUFfjuMVVc","executionInfo":{"status":"ok","timestamp":1719651172464,"user_tz":-60,"elapsed":7803,"user":{"displayName":"Rupert Murphy","userId":"07708960882595328857"}},"outputId":"94310d56-f058-43b0-f023-989fe76132c0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: modin[all] in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Requirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2.2.2)\n","Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (24.1)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (1.25.2)\n","Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2023.6.0)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (5.9.5)\n","Requirement already satisfied: dask>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2023.8.1)\n","Requirement already satisfied: distributed>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2023.8.1)\n","Requirement already satisfied: ray!=2.5.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (2.31.0)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (14.0.2)\n","Requirement already satisfied: modin-spreadsheet>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (0.1.2)\n","Requirement already satisfied: dataframe-api-compat>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from modin[all]) (0.2.7)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (8.1.7)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (2.2.1)\n","Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (6.0.1)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (0.12.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.22.0->modin[all]) (7.2.0)\n","Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (3.1.4)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (1.0.8)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (3.0.0)\n","Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (6.3.3)\n","Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (2.0.7)\n","Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.22.0->modin[all]) (3.0.0)\n","Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from modin-spreadsheet>=0.1.0->modin[all]) (1.0.0)\n","Requirement already satisfied: notebook>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from modin-spreadsheet>=0.1.0->modin[all]) (6.5.5)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from modin-spreadsheet>=0.1.0->modin[all]) (7.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[all]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[all]) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[all]) (2024.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (3.15.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (4.19.2)\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (3.20.3)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (1.3.1)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[all]) (2.31.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2.22.0->modin[all]) (3.19.2)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (5.5.6)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (3.6.6)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (7.34.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (3.0.11)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2.22.0->modin[all]) (2.1.5)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (5.5.2)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (6.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (6.5.4)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (23.1.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (5.7.2)\n","Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (6.1.12)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (5.10.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.20.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->modin[all]) (1.16.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[all]) (0.18.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[all]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[all]) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[all]) (2024.6.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (67.7.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (3.0.47)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.9.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (4.2.2)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.2.4)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (1.3.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (2.20.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (21.2.0)\n","Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (2.4.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.8.4)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.24.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.2.13)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->modin-spreadsheet>=0.1.0->modin[all]) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (2.22)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.8.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0.3->modin-spreadsheet>=0.1.0->modin[all]) (1.2.1)\n"]}]}]}